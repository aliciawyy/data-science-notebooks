{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import skew, boxcox\n",
    "from IPython.core.pylabtools import figsize\n",
    "import xgboost as xgb\n",
    "%matplotlib inline\n",
    "\n",
    "from os import path\n",
    "to_filename = lambda name: path.join(\"..\", \"data\", \"allstate\", name +\".csv\")\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: train (188318, 131), test (125546, 130)\n",
      "   cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 cat10   ...        cont6  \\\n",
      "id                                                      ...                \n",
      "1     A    B    A    B    A    A    A    A    B     A   ...     0.718367   \n",
      "2     A    B    A    A    A    A    A    A    B     B   ...     0.438917   \n",
      "\n",
      "       cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
      "id                                                                      \n",
      "1   0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
      "2   0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
      "\n",
      "      cont14     loss  \n",
      "id                     \n",
      "1   0.714843  2213.18  \n",
      "2   0.304496  1283.60  \n",
      "\n",
      "[2 rows x 131 columns]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(to_filename(\"train\"), index_col=0)\n",
    "test = pd.read_csv(to_filename(\"test\"), index_col=0)\n",
    "print(\"shape: train {}, test {}\".format(train.shape, test.shape))\n",
    "print(train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = np.log(train.loss)\n",
    "\n",
    "mean_resp = np.mean(response)\n",
    "std_resp = np.std(response)\n",
    "response = (response - mean_resp) / std_resp\n",
    "\n",
    "\n",
    "def restore_pred1(y):\n",
    "    return np.exp(y)\n",
    "\n",
    "def restore_pred(y):\n",
    "    return np.exp(y * std_resp + mean_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Categorical columns:', ['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cat19', 'cat20', 'cat21', 'cat22', 'cat23', 'cat24', 'cat25', 'cat26', 'cat27', 'cat28', 'cat29', 'cat30', 'cat31', 'cat32', 'cat33', 'cat34', 'cat35', 'cat36', 'cat37', 'cat38', 'cat39', 'cat40', 'cat41', 'cat42', 'cat43', 'cat44', 'cat45', 'cat46', 'cat47', 'cat48', 'cat49', 'cat50', 'cat51', 'cat52', 'cat53', 'cat54', 'cat55', 'cat56', 'cat57', 'cat58', 'cat59', 'cat60', 'cat61', 'cat62', 'cat63', 'cat64', 'cat65', 'cat66', 'cat67', 'cat68', 'cat69', 'cat70', 'cat71', 'cat72', 'cat73', 'cat74', 'cat75', 'cat76', 'cat77', 'cat78', 'cat79', 'cat80', 'cat81', 'cat82', 'cat83', 'cat84', 'cat85', 'cat86', 'cat87', 'cat88', 'cat89', 'cat90', 'cat91', 'cat92', 'cat93', 'cat94', 'cat95', 'cat96', 'cat97', 'cat98', 'cat99', 'cat100', 'cat101', 'cat102', 'cat103', 'cat104', 'cat105', 'cat106', 'cat107', 'cat108', 'cat109', 'cat110', 'cat111', 'cat112', 'cat113', 'cat114', 'cat115', 'cat116'])\n"
     ]
    }
   ],
   "source": [
    "cat_features = [col for col in train.columns if col.startswith(\"cat\")]\n",
    "print(\"Categorical columns:\", cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorical features preprocessing\n",
    "# Method 1: Encoding categorical features into int\n",
    "for col in cat_features:\n",
    "    encd = preprocessing.LabelEncoder()\n",
    "    encd.fit(train[col].value_counts().index.union(test[col].value_counts().index))\n",
    "    train[col] = encd.transform(train[col])\n",
    "    test[col] = encd.transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method 2: Using cardinal features for categorical features\n",
    "col = cat_features[0]\n",
    "test_col = train[col][:10].copy()\n",
    "for col in cat_features:\n",
    "    key_map = response.groupby(train[col]).mean().to_dict()\n",
    "    train[col] = train[col].replace(key_map)\n",
    "    for k in set(test[col].value_counts().index).difference(key_map.keys()):\n",
    "        key_map[k] = np.NAN\n",
    "    test[col] = test[col].replace(key_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Numerical columns:', ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14'])\n"
     ]
    }
   ],
   "source": [
    "# preprocess numerical features\n",
    "num_features = [col for col in train.columns if col.startswith(\"cont\")]\n",
    "print(\"Numerical columns:\", num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method 1: Standard Scaler\n",
    "for col in num_features:\n",
    "    sc = preprocessing.StandardScaler()\n",
    "    train[col] = sc.fit_transform(train[[col]])\n",
    "    test[col] = sc.transform(test[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Skew in numeric features:', cont1     0.513205\n",
      "cont2    -0.311146\n",
      "cont3    -0.007023\n",
      "cont4     0.417559\n",
      "cont5     0.679610\n",
      "cont6     0.458413\n",
      "cont7     0.825889\n",
      "cont8     0.673237\n",
      "cont9     1.067247\n",
      "cont10    0.352116\n",
      "cont11    0.281139\n",
      "cont12    0.291997\n",
      "cont13    0.376138\n",
      "cont14    0.250673\n",
      "dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "# study the skewness in the numerical features\n",
    "skewed_feats = pd.concat([train[num_features], test[num_features]]).apply(lambda x: skew(x.dropna()))\n",
    "print(\"Skew in numeric features:\", skewed_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method 2: Box-Cox transformation when numerical feature skewness > .25\n",
    "for feat in skewed_feats[skewed_feats > 0.25].index:\n",
    "    train[feat], lam = boxcox(train[feat] + 1.)\n",
    "    test[feat], lam = boxcox(test[feat] + 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train.drop(\"loss\", 1), response)\n",
    "dtest = xgb.DMatrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'objective':\"reg:linear\", 'silent': True, 'max_depth': 7, 'min_child_weight': 1,\n",
    "          'colsample_bytree': .7, \"subsample\": 1., 'eta': 0.1, 'eval_metric':'mae',# \"n_estimators\": 20,\n",
    "          \"gamma\": 0.5, \"lambda\": 0.8, \"silent\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    test-mae-mean  test-mae-std  train-mae-mean  train-mae-std\n",
      "0        0.791689      0.003139        0.790803       0.001235\n",
      "1        0.711323      0.003149        0.709352       0.002114\n",
      "2        0.656812      0.002042        0.653948       0.001794\n",
      "3        0.620731      0.002414        0.616826       0.000628\n",
      "4        0.596710      0.002187        0.591768       0.000454\n",
      "5        0.580212      0.002258        0.574466       0.000412\n",
      "6        0.568539      0.002131        0.561919       0.000628\n",
      "7        0.559974      0.001806        0.552459       0.000483\n",
      "8        0.553738      0.001513        0.545400       0.000547\n",
      "9        0.548844      0.001217        0.539716       0.000382\n",
      "10       0.545042      0.000931        0.534977       0.000428\n",
      "11       0.541749      0.000705        0.530983       0.000334\n",
      "12       0.539042      0.000683        0.527467       0.000154\n",
      "13       0.536925      0.000308        0.524565       0.000420\n",
      "14       0.534990      0.000514        0.521948       0.000335\n",
      "15       0.533025      0.000648        0.519322       0.000137\n",
      "16       0.531993      0.000493        0.517571       0.000114\n",
      "17       0.530693      0.000685        0.515604       0.000314\n",
      "18       0.529808      0.000563        0.514021       0.000241\n",
      "19       0.528749      0.000529        0.512281       0.000110\n",
      "20       0.527937      0.000498        0.510914       0.000130\n",
      "21       0.527112      0.000378        0.509560       0.000138\n",
      "22       0.526319      0.000614        0.508201       0.000432\n",
      "23       0.525778      0.000768        0.507099       0.000510\n",
      "24       0.525414      0.000763        0.506048       0.000583\n",
      "25       0.524883      0.000757        0.505008       0.000567\n",
      "26       0.524317      0.000710        0.503825       0.000489\n",
      "27       0.523785      0.000524        0.502870       0.000400\n",
      "28       0.523448      0.000407        0.502095       0.000187\n",
      "29       0.523113      0.000445        0.501261       0.000264\n",
      "30       0.522691      0.000354        0.500269       0.000086\n",
      "31       0.522414      0.000384        0.499515       0.000157\n",
      "32       0.522031      0.000457        0.498738       0.000275\n",
      "33       0.521839      0.000441        0.498179       0.000314\n",
      "34       0.521533      0.000319        0.497336       0.000271\n",
      "35       0.521337      0.000303        0.496692       0.000336\n",
      "36       0.521206      0.000349        0.496116       0.000283\n",
      "37       0.520958      0.000425        0.495288       0.000391\n",
      "38       0.520750      0.000421        0.494591       0.000490\n",
      "39       0.520581      0.000502        0.493984       0.000618\n",
      "40       0.520438      0.000550        0.493436       0.000637\n",
      "41       0.520248      0.000542        0.492737       0.000589\n",
      "42       0.520099      0.000488        0.492127       0.000469\n",
      "43       0.520004      0.000458        0.491683       0.000433\n",
      "44       0.519896      0.000391        0.491062       0.000308\n",
      "45       0.519784      0.000346        0.490544       0.000333\n",
      "46       0.519723      0.000317        0.490089       0.000299\n",
      "47       0.519630      0.000316        0.489656       0.000314\n",
      "48       0.519583      0.000248        0.489100       0.000414\n",
      "49       0.519441      0.000239        0.488439       0.000250\n"
     ]
    }
   ],
   "source": [
    "cvresult = xgb.cv(params, dtrain, nfold=3, num_boost_round=50)\n",
    "print(cvresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvresult[[\"test-mae-mean\", \"train-mae-mean\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvresult[[\"test-mae-mean\", \"train-mae-mean\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.862118\teval-mae:0.863862\n",
      "[1]\ttrain-mae:0.817106\teval-mae:0.81905\n",
      "[2]\ttrain-mae:0.778025\teval-mae:0.780266\n",
      "[3]\ttrain-mae:0.744741\teval-mae:0.747218\n",
      "[4]\ttrain-mae:0.715918\teval-mae:0.718649\n",
      "[5]\ttrain-mae:0.692098\teval-mae:0.694921\n",
      "[6]\ttrain-mae:0.67084\teval-mae:0.673868\n",
      "[7]\ttrain-mae:0.65255\teval-mae:0.655659\n",
      "[8]\ttrain-mae:0.637208\teval-mae:0.640569\n",
      "[9]\ttrain-mae:0.624119\teval-mae:0.627853\n",
      "[10]\ttrain-mae:0.612469\teval-mae:0.61644\n",
      "[11]\ttrain-mae:0.602162\teval-mae:0.606215\n",
      "[12]\ttrain-mae:0.593057\teval-mae:0.597417\n",
      "[13]\ttrain-mae:0.58527\teval-mae:0.589938\n",
      "[14]\ttrain-mae:0.578482\teval-mae:0.583359\n",
      "[15]\ttrain-mae:0.572265\teval-mae:0.577558\n",
      "[16]\ttrain-mae:0.56706\teval-mae:0.57257\n",
      "[17]\ttrain-mae:0.562433\teval-mae:0.568329\n",
      "[18]\ttrain-mae:0.557944\teval-mae:0.564207\n",
      "[19]\ttrain-mae:0.554005\teval-mae:0.560586\n",
      "[20]\ttrain-mae:0.550594\teval-mae:0.557621\n",
      "[21]\ttrain-mae:0.547647\teval-mae:0.554989\n",
      "[22]\ttrain-mae:0.544768\teval-mae:0.552466\n",
      "[23]\ttrain-mae:0.54236\teval-mae:0.550468\n",
      "[24]\ttrain-mae:0.540181\teval-mae:0.548575\n",
      "[25]\ttrain-mae:0.538105\teval-mae:0.5467\n",
      "[26]\ttrain-mae:0.535988\teval-mae:0.544875\n",
      "[27]\ttrain-mae:0.534041\teval-mae:0.543193\n",
      "[28]\ttrain-mae:0.532405\teval-mae:0.541814\n",
      "[29]\ttrain-mae:0.530644\teval-mae:0.540335\n",
      "[30]\ttrain-mae:0.529159\teval-mae:0.539102\n",
      "[31]\ttrain-mae:0.527656\teval-mae:0.537862\n",
      "[32]\ttrain-mae:0.526316\teval-mae:0.536889\n",
      "[33]\ttrain-mae:0.525203\teval-mae:0.536056\n",
      "[34]\ttrain-mae:0.523982\teval-mae:0.535134\n",
      "[35]\ttrain-mae:0.522851\teval-mae:0.534364\n",
      "[36]\ttrain-mae:0.521705\teval-mae:0.533462\n",
      "[37]\ttrain-mae:0.519861\teval-mae:0.531893\n",
      "[38]\ttrain-mae:0.518803\teval-mae:0.531062\n",
      "[39]\ttrain-mae:0.517987\teval-mae:0.530452\n",
      "[40]\ttrain-mae:0.517162\teval-mae:0.52988\n",
      "[41]\ttrain-mae:0.516252\teval-mae:0.529279\n",
      "[42]\ttrain-mae:0.515509\teval-mae:0.528747\n",
      "[43]\ttrain-mae:0.514694\teval-mae:0.52825\n",
      "[44]\ttrain-mae:0.513871\teval-mae:0.527696\n",
      "[45]\ttrain-mae:0.513116\teval-mae:0.527203\n",
      "[46]\ttrain-mae:0.512525\teval-mae:0.526819\n",
      "[47]\ttrain-mae:0.511913\teval-mae:0.526506\n",
      "[48]\ttrain-mae:0.511175\teval-mae:0.526052\n",
      "[49]\ttrain-mae:0.510558\teval-mae:0.525675\n",
      "[50]\ttrain-mae:0.509685\teval-mae:0.524984\n",
      "[51]\ttrain-mae:0.509152\teval-mae:0.524664\n",
      "[52]\ttrain-mae:0.508696\teval-mae:0.524403\n",
      "[53]\ttrain-mae:0.507968\teval-mae:0.523877\n",
      "[54]\ttrain-mae:0.507476\teval-mae:0.523609\n",
      "[55]\ttrain-mae:0.506854\teval-mae:0.523192\n",
      "[56]\ttrain-mae:0.506409\teval-mae:0.522946\n",
      "[57]\ttrain-mae:0.505933\teval-mae:0.522737\n",
      "[58]\ttrain-mae:0.505246\teval-mae:0.522331\n",
      "[59]\ttrain-mae:0.504779\teval-mae:0.522146\n",
      "[60]\ttrain-mae:0.504295\teval-mae:0.521889\n",
      "[61]\ttrain-mae:0.503757\teval-mae:0.521557\n",
      "[62]\ttrain-mae:0.503332\teval-mae:0.521396\n",
      "[63]\ttrain-mae:0.502865\teval-mae:0.521119\n",
      "[64]\ttrain-mae:0.502395\teval-mae:0.520871\n",
      "[65]\ttrain-mae:0.502048\teval-mae:0.520705\n",
      "[66]\ttrain-mae:0.501478\teval-mae:0.520362\n",
      "[67]\ttrain-mae:0.50112\teval-mae:0.520247\n",
      "[68]\ttrain-mae:0.500773\teval-mae:0.520167\n",
      "[69]\ttrain-mae:0.500397\teval-mae:0.519999\n",
      "[70]\ttrain-mae:0.500003\teval-mae:0.519828\n",
      "[71]\ttrain-mae:0.499669\teval-mae:0.519701\n",
      "[72]\ttrain-mae:0.499307\teval-mae:0.519526\n",
      "[73]\ttrain-mae:0.498972\teval-mae:0.519386\n",
      "[74]\ttrain-mae:0.498601\teval-mae:0.519235\n",
      "[75]\ttrain-mae:0.498271\teval-mae:0.519047\n",
      "[76]\ttrain-mae:0.497978\teval-mae:0.518971\n",
      "[77]\ttrain-mae:0.497598\teval-mae:0.518791\n",
      "[78]\ttrain-mae:0.497317\teval-mae:0.518714\n",
      "[79]\ttrain-mae:0.497004\teval-mae:0.518561\n",
      "[80]\ttrain-mae:0.496788\teval-mae:0.518481\n",
      "[81]\ttrain-mae:0.496462\teval-mae:0.518373\n",
      "[82]\ttrain-mae:0.49608\teval-mae:0.518233\n",
      "[83]\ttrain-mae:0.495713\teval-mae:0.518071\n",
      "[84]\ttrain-mae:0.495458\teval-mae:0.517981\n",
      "[85]\ttrain-mae:0.495177\teval-mae:0.517911\n",
      "[86]\ttrain-mae:0.494859\teval-mae:0.517787\n",
      "[87]\ttrain-mae:0.49464\teval-mae:0.517718\n",
      "[88]\ttrain-mae:0.494441\teval-mae:0.517671\n",
      "[89]\ttrain-mae:0.494108\teval-mae:0.517559\n",
      "[90]\ttrain-mae:0.493832\teval-mae:0.517436\n",
      "[91]\ttrain-mae:0.493529\teval-mae:0.517339\n",
      "[92]\ttrain-mae:0.49336\teval-mae:0.517311\n",
      "[93]\ttrain-mae:0.492921\teval-mae:0.517094\n",
      "[94]\ttrain-mae:0.492637\teval-mae:0.516982\n",
      "[95]\ttrain-mae:0.492309\teval-mae:0.516896\n",
      "[96]\ttrain-mae:0.492093\teval-mae:0.516832\n",
      "[97]\ttrain-mae:0.491744\teval-mae:0.516729\n",
      "[98]\ttrain-mae:0.49157\teval-mae:0.516684\n",
      "[99]\ttrain-mae:0.491222\teval-mae:0.516576\n",
      "[100]\ttrain-mae:0.49107\teval-mae:0.516556\n",
      "[101]\ttrain-mae:0.490823\teval-mae:0.516516\n",
      "[102]\ttrain-mae:0.49058\teval-mae:0.516426\n",
      "[103]\ttrain-mae:0.490345\teval-mae:0.516407\n",
      "[104]\ttrain-mae:0.490061\teval-mae:0.516326\n",
      "[105]\ttrain-mae:0.489819\teval-mae:0.51627\n",
      "[106]\ttrain-mae:0.489559\teval-mae:0.51621\n",
      "[107]\ttrain-mae:0.489351\teval-mae:0.51609\n",
      "[108]\ttrain-mae:0.489098\teval-mae:0.516008\n",
      "[109]\ttrain-mae:0.488959\teval-mae:0.515989\n",
      "[110]\ttrain-mae:0.488707\teval-mae:0.515898\n",
      "[111]\ttrain-mae:0.488426\teval-mae:0.515811\n",
      "[112]\ttrain-mae:0.48816\teval-mae:0.515712\n",
      "[113]\ttrain-mae:0.487923\teval-mae:0.515652\n",
      "[114]\ttrain-mae:0.487679\teval-mae:0.515598\n",
      "[115]\ttrain-mae:0.487324\teval-mae:0.515465\n",
      "[116]\ttrain-mae:0.487053\teval-mae:0.51538\n",
      "[117]\ttrain-mae:0.486884\teval-mae:0.515341\n",
      "[118]\ttrain-mae:0.486689\teval-mae:0.515316\n",
      "[119]\ttrain-mae:0.486616\teval-mae:0.51529\n",
      "[120]\ttrain-mae:0.486296\teval-mae:0.515209\n",
      "[121]\ttrain-mae:0.486091\teval-mae:0.515165\n",
      "[122]\ttrain-mae:0.485953\teval-mae:0.515141\n",
      "[123]\ttrain-mae:0.48573\teval-mae:0.51513\n",
      "[124]\ttrain-mae:0.485559\teval-mae:0.515102\n",
      "[125]\ttrain-mae:0.485422\teval-mae:0.515062\n",
      "[126]\ttrain-mae:0.485211\teval-mae:0.515078\n",
      "[127]\ttrain-mae:0.485031\teval-mae:0.515034\n",
      "[128]\ttrain-mae:0.484863\teval-mae:0.514997\n",
      "[129]\ttrain-mae:0.484679\teval-mae:0.514952\n",
      "[130]\ttrain-mae:0.484439\teval-mae:0.514925\n",
      "[131]\ttrain-mae:0.484145\teval-mae:0.51482\n",
      "[132]\ttrain-mae:0.484024\teval-mae:0.514784\n",
      "[133]\ttrain-mae:0.4839\teval-mae:0.514768\n",
      "[134]\ttrain-mae:0.483612\teval-mae:0.514742\n",
      "[135]\ttrain-mae:0.483355\teval-mae:0.514702\n",
      "[136]\ttrain-mae:0.483107\teval-mae:0.514625\n",
      "[137]\ttrain-mae:0.482881\teval-mae:0.514578\n",
      "[138]\ttrain-mae:0.482816\teval-mae:0.514545\n",
      "[139]\ttrain-mae:0.482679\teval-mae:0.514488\n",
      "[140]\ttrain-mae:0.482544\teval-mae:0.514483\n",
      "[141]\ttrain-mae:0.482391\teval-mae:0.514435\n",
      "[142]\ttrain-mae:0.482184\teval-mae:0.514409\n",
      "[143]\ttrain-mae:0.482004\teval-mae:0.514412\n",
      "[144]\ttrain-mae:0.481821\teval-mae:0.514398\n",
      "[145]\ttrain-mae:0.481527\teval-mae:0.514385\n",
      "[146]\ttrain-mae:0.481398\teval-mae:0.514358\n",
      "[147]\ttrain-mae:0.481183\teval-mae:0.514344\n",
      "[148]\ttrain-mae:0.480993\teval-mae:0.514291\n",
      "[149]\ttrain-mae:0.480878\teval-mae:0.514264\n",
      "[150]\ttrain-mae:0.480567\teval-mae:0.514248\n",
      "[151]\ttrain-mae:0.480313\teval-mae:0.514228\n",
      "[152]\ttrain-mae:0.480178\teval-mae:0.514189\n",
      "[153]\ttrain-mae:0.479961\teval-mae:0.514191\n",
      "[154]\ttrain-mae:0.479725\teval-mae:0.514137\n",
      "[155]\ttrain-mae:0.47953\teval-mae:0.514108\n",
      "[156]\ttrain-mae:0.479475\teval-mae:0.514089\n",
      "[157]\ttrain-mae:0.479391\teval-mae:0.5141\n",
      "[158]\ttrain-mae:0.479287\teval-mae:0.514082\n",
      "[159]\ttrain-mae:0.479115\teval-mae:0.514034\n",
      "[160]\ttrain-mae:0.478972\teval-mae:0.513993\n",
      "[161]\ttrain-mae:0.478639\teval-mae:0.513852\n",
      "[162]\ttrain-mae:0.478517\teval-mae:0.513809\n",
      "[163]\ttrain-mae:0.478333\teval-mae:0.513764\n",
      "[164]\ttrain-mae:0.478156\teval-mae:0.513785\n",
      "[165]\ttrain-mae:0.477933\teval-mae:0.513767\n",
      "[166]\ttrain-mae:0.47776\teval-mae:0.513726\n",
      "[167]\ttrain-mae:0.477476\teval-mae:0.513709\n",
      "[168]\ttrain-mae:0.477401\teval-mae:0.513732\n",
      "[169]\ttrain-mae:0.477311\teval-mae:0.513705\n",
      "[170]\ttrain-mae:0.477057\teval-mae:0.513682\n",
      "[171]\ttrain-mae:0.476974\teval-mae:0.513677\n",
      "[172]\ttrain-mae:0.476838\teval-mae:0.513678\n",
      "[173]\ttrain-mae:0.476748\teval-mae:0.51365\n",
      "[174]\ttrain-mae:0.47658\teval-mae:0.513641\n",
      "[175]\ttrain-mae:0.476217\teval-mae:0.513511\n",
      "[176]\ttrain-mae:0.476186\teval-mae:0.513494\n",
      "[177]\ttrain-mae:0.476\teval-mae:0.51347\n",
      "[178]\ttrain-mae:0.475817\teval-mae:0.51345\n",
      "[179]\ttrain-mae:0.4757\teval-mae:0.513442\n",
      "[180]\ttrain-mae:0.475372\teval-mae:0.513372\n",
      "[181]\ttrain-mae:0.475112\teval-mae:0.513308\n",
      "[182]\ttrain-mae:0.474945\teval-mae:0.513284\n",
      "[183]\ttrain-mae:0.474756\teval-mae:0.513261\n",
      "[184]\ttrain-mae:0.47453\teval-mae:0.513238\n",
      "[185]\ttrain-mae:0.474379\teval-mae:0.513206\n",
      "[186]\ttrain-mae:0.474226\teval-mae:0.513194\n",
      "[187]\ttrain-mae:0.474109\teval-mae:0.513166\n",
      "[188]\ttrain-mae:0.473828\teval-mae:0.513101\n",
      "[189]\ttrain-mae:0.473773\teval-mae:0.513094\n",
      "[190]\ttrain-mae:0.473646\teval-mae:0.513085\n",
      "[191]\ttrain-mae:0.473393\teval-mae:0.513044\n",
      "[192]\ttrain-mae:0.473321\teval-mae:0.513051\n",
      "[193]\ttrain-mae:0.473194\teval-mae:0.513047\n",
      "[194]\ttrain-mae:0.473124\teval-mae:0.513034\n",
      "[195]\ttrain-mae:0.472984\teval-mae:0.513021\n",
      "[196]\ttrain-mae:0.472851\teval-mae:0.512979\n",
      "[197]\ttrain-mae:0.472678\teval-mae:0.512987\n",
      "[198]\ttrain-mae:0.472513\teval-mae:0.512995\n",
      "[199]\ttrain-mae:0.472462\teval-mae:0.512995\n",
      "[200]\ttrain-mae:0.472354\teval-mae:0.51299\n",
      "[201]\ttrain-mae:0.472276\teval-mae:0.512962\n",
      "[202]\ttrain-mae:0.472114\teval-mae:0.512928\n",
      "[203]\ttrain-mae:0.472005\teval-mae:0.512936\n",
      "[204]\ttrain-mae:0.471858\teval-mae:0.512893\n",
      "[205]\ttrain-mae:0.47159\teval-mae:0.51279\n",
      "[206]\ttrain-mae:0.471492\teval-mae:0.512769\n",
      "[207]\ttrain-mae:0.471192\teval-mae:0.512738\n",
      "[208]\ttrain-mae:0.47104\teval-mae:0.512734\n",
      "[209]\ttrain-mae:0.470829\teval-mae:0.512726\n",
      "[210]\ttrain-mae:0.470716\teval-mae:0.51273\n",
      "[211]\ttrain-mae:0.470687\teval-mae:0.512713\n",
      "[212]\ttrain-mae:0.470583\teval-mae:0.51272\n",
      "[213]\ttrain-mae:0.470512\teval-mae:0.512711\n",
      "[214]\ttrain-mae:0.470276\teval-mae:0.512639\n",
      "[215]\ttrain-mae:0.470127\teval-mae:0.512643\n",
      "[216]\ttrain-mae:0.469968\teval-mae:0.512629\n",
      "[217]\ttrain-mae:0.469866\teval-mae:0.512641\n",
      "[218]\ttrain-mae:0.469765\teval-mae:0.512636\n",
      "[219]\ttrain-mae:0.469574\teval-mae:0.512639\n",
      "[220]\ttrain-mae:0.469475\teval-mae:0.512632\n",
      "[221]\ttrain-mae:0.469296\teval-mae:0.512619\n",
      "[222]\ttrain-mae:0.469238\teval-mae:0.512621\n",
      "[223]\ttrain-mae:0.469023\teval-mae:0.512571\n",
      "[224]\ttrain-mae:0.468919\teval-mae:0.512546\n",
      "[225]\ttrain-mae:0.468771\teval-mae:0.512523\n",
      "[226]\ttrain-mae:0.468758\teval-mae:0.512528\n",
      "[227]\ttrain-mae:0.468636\teval-mae:0.512535\n",
      "[228]\ttrain-mae:0.468417\teval-mae:0.512524\n",
      "[229]\ttrain-mae:0.468242\teval-mae:0.5125\n",
      "[230]\ttrain-mae:0.46799\teval-mae:0.512429\n",
      "[231]\ttrain-mae:0.467801\teval-mae:0.512419\n",
      "[232]\ttrain-mae:0.467651\teval-mae:0.512407\n",
      "[233]\ttrain-mae:0.467632\teval-mae:0.512408\n",
      "[234]\ttrain-mae:0.467426\teval-mae:0.512378\n",
      "[235]\ttrain-mae:0.467358\teval-mae:0.512382\n",
      "[236]\ttrain-mae:0.467321\teval-mae:0.512384\n",
      "[237]\ttrain-mae:0.467179\teval-mae:0.512401\n",
      "[238]\ttrain-mae:0.467096\teval-mae:0.512403\n",
      "[239]\ttrain-mae:0.467016\teval-mae:0.512374\n",
      "[240]\ttrain-mae:0.467007\teval-mae:0.51237\n",
      "[241]\ttrain-mae:0.466937\teval-mae:0.512368\n",
      "[242]\ttrain-mae:0.466907\teval-mae:0.512369\n",
      "[243]\ttrain-mae:0.466828\teval-mae:0.512361\n",
      "[244]\ttrain-mae:0.466696\teval-mae:0.512335\n",
      "[245]\ttrain-mae:0.466444\teval-mae:0.512284\n",
      "[246]\ttrain-mae:0.466354\teval-mae:0.512297\n",
      "[247]\ttrain-mae:0.46623\teval-mae:0.512298\n",
      "[248]\ttrain-mae:0.46608\teval-mae:0.512302\n",
      "[249]\ttrain-mae:0.465901\teval-mae:0.512301\n",
      "[250]\ttrain-mae:0.46568\teval-mae:0.512294\n",
      "[251]\ttrain-mae:0.465518\teval-mae:0.51228\n",
      "[252]\ttrain-mae:0.465346\teval-mae:0.512267\n",
      "[253]\ttrain-mae:0.465253\teval-mae:0.512272\n",
      "[254]\ttrain-mae:0.465101\teval-mae:0.512236\n",
      "[255]\ttrain-mae:0.464916\teval-mae:0.512218\n",
      "[256]\ttrain-mae:0.464859\teval-mae:0.512226\n",
      "[257]\ttrain-mae:0.464697\teval-mae:0.51222\n",
      "[258]\ttrain-mae:0.464628\teval-mae:0.512224\n",
      "[259]\ttrain-mae:0.464439\teval-mae:0.512218\n",
      "[260]\ttrain-mae:0.46427\teval-mae:0.512241\n",
      "[261]\ttrain-mae:0.46423\teval-mae:0.512233\n",
      "[262]\ttrain-mae:0.46402\teval-mae:0.512217\n",
      "[263]\ttrain-mae:0.463795\teval-mae:0.51222\n",
      "[264]\ttrain-mae:0.463558\teval-mae:0.512207\n",
      "[265]\ttrain-mae:0.463458\teval-mae:0.512222\n",
      "[266]\ttrain-mae:0.46328\teval-mae:0.512234\n",
      "[267]\ttrain-mae:0.463187\teval-mae:0.512239\n",
      "[268]\ttrain-mae:0.463093\teval-mae:0.512245\n",
      "[269]\ttrain-mae:0.463026\teval-mae:0.512237\n",
      "[270]\ttrain-mae:0.462957\teval-mae:0.512241\n",
      "[271]\ttrain-mae:0.46285\teval-mae:0.512237\n",
      "[272]\ttrain-mae:0.462706\teval-mae:0.512248\n",
      "[273]\ttrain-mae:0.462533\teval-mae:0.512252\n",
      "[274]\ttrain-mae:0.462359\teval-mae:0.512254\n",
      "[275]\ttrain-mae:0.462222\teval-mae:0.512217\n",
      "[276]\ttrain-mae:0.462132\teval-mae:0.512218\n",
      "[277]\ttrain-mae:0.461964\teval-mae:0.512194\n",
      "[278]\ttrain-mae:0.461906\teval-mae:0.512198\n",
      "[279]\ttrain-mae:0.461773\teval-mae:0.512196\n",
      "[280]\ttrain-mae:0.461748\teval-mae:0.512194\n",
      "[281]\ttrain-mae:0.461673\teval-mae:0.512188\n",
      "[282]\ttrain-mae:0.461495\teval-mae:0.512205\n",
      "[283]\ttrain-mae:0.461302\teval-mae:0.512184\n",
      "[284]\ttrain-mae:0.461149\teval-mae:0.512185\n",
      "[285]\ttrain-mae:0.46113\teval-mae:0.512183\n",
      "[286]\ttrain-mae:0.46101\teval-mae:0.512192\n",
      "[287]\ttrain-mae:0.460945\teval-mae:0.512198\n",
      "[288]\ttrain-mae:0.460846\teval-mae:0.512191\n",
      "[289]\ttrain-mae:0.460838\teval-mae:0.512194\n",
      "[290]\ttrain-mae:0.460778\teval-mae:0.512195\n",
      "[291]\ttrain-mae:0.460637\teval-mae:0.512216\n",
      "[292]\ttrain-mae:0.460541\teval-mae:0.512234\n",
      "[293]\ttrain-mae:0.460462\teval-mae:0.512236\n",
      "[294]\ttrain-mae:0.460242\teval-mae:0.512227\n",
      "[295]\ttrain-mae:0.460084\teval-mae:0.512243\n",
      "[296]\ttrain-mae:0.459989\teval-mae:0.512259\n",
      "[297]\ttrain-mae:0.459877\teval-mae:0.512248\n",
      "[298]\ttrain-mae:0.459769\teval-mae:0.512259\n",
      "[299]\ttrain-mae:0.459633\teval-mae:0.512264\n",
      "('mae for all train', 0, 1053.3167954612784)\n",
      "[0]\ttrain-mae:0.865681\teval-mae:0.866372\n",
      "[1]\ttrain-mae:0.822777\teval-mae:0.823539\n",
      "[2]\ttrain-mae:0.78315\teval-mae:0.784072\n",
      "[3]\ttrain-mae:0.748496\teval-mae:0.7496\n",
      "[4]\ttrain-mae:0.720533\teval-mae:0.721965\n",
      "[5]\ttrain-mae:0.69485\teval-mae:0.696887\n",
      "[6]\ttrain-mae:0.672584\teval-mae:0.67534\n",
      "[7]\ttrain-mae:0.654728\teval-mae:0.657952\n",
      "[8]\ttrain-mae:0.638557\teval-mae:0.642317\n",
      "[9]\ttrain-mae:0.624499\teval-mae:0.628888\n",
      "[10]\ttrain-mae:0.611995\teval-mae:0.616988\n",
      "[11]\ttrain-mae:0.601882\teval-mae:0.60741\n",
      "[12]\ttrain-mae:0.592616\teval-mae:0.598675\n",
      "[13]\ttrain-mae:0.585046\teval-mae:0.591646\n",
      "[14]\ttrain-mae:0.577899\teval-mae:0.584936\n",
      "[15]\ttrain-mae:0.571277\teval-mae:0.578744\n",
      "[16]\ttrain-mae:0.565754\teval-mae:0.573575\n",
      "[17]\ttrain-mae:0.560938\teval-mae:0.569353\n",
      "[18]\ttrain-mae:0.556748\teval-mae:0.565537\n",
      "[19]\ttrain-mae:0.552693\teval-mae:0.56191\n",
      "[20]\ttrain-mae:0.548997\teval-mae:0.558582\n",
      "[21]\ttrain-mae:0.546018\teval-mae:0.55593\n",
      "[22]\ttrain-mae:0.543188\teval-mae:0.553472\n",
      "[23]\ttrain-mae:0.540429\teval-mae:0.551115\n",
      "[24]\ttrain-mae:0.538073\teval-mae:0.549069\n",
      "[25]\ttrain-mae:0.536105\teval-mae:0.54739\n",
      "[26]\ttrain-mae:0.534109\teval-mae:0.545747\n",
      "[27]\ttrain-mae:0.531988\teval-mae:0.543911\n",
      "[28]\ttrain-mae:0.530315\teval-mae:0.54264\n",
      "[29]\ttrain-mae:0.528564\teval-mae:0.541214\n",
      "[30]\ttrain-mae:0.527153\teval-mae:0.540139\n",
      "[31]\ttrain-mae:0.525573\teval-mae:0.538878\n",
      "[32]\ttrain-mae:0.524189\teval-mae:0.537832\n",
      "[33]\ttrain-mae:0.522944\teval-mae:0.53691\n",
      "[34]\ttrain-mae:0.521731\teval-mae:0.536018\n",
      "[35]\ttrain-mae:0.520457\teval-mae:0.535052\n",
      "[36]\ttrain-mae:0.519377\teval-mae:0.534264\n",
      "[37]\ttrain-mae:0.517987\teval-mae:0.53321\n",
      "[38]\ttrain-mae:0.517062\teval-mae:0.532614\n",
      "[39]\ttrain-mae:0.516181\teval-mae:0.532035\n",
      "[40]\ttrain-mae:0.515309\teval-mae:0.5315\n",
      "[41]\ttrain-mae:0.514228\teval-mae:0.530765\n",
      "[42]\ttrain-mae:0.513439\teval-mae:0.530235\n",
      "[43]\ttrain-mae:0.512668\teval-mae:0.529707\n",
      "[44]\ttrain-mae:0.511958\teval-mae:0.529252\n",
      "[45]\ttrain-mae:0.511009\teval-mae:0.528607\n",
      "[46]\ttrain-mae:0.510378\teval-mae:0.528235\n",
      "[47]\ttrain-mae:0.509748\teval-mae:0.527863\n",
      "[48]\ttrain-mae:0.508901\teval-mae:0.527234\n",
      "[49]\ttrain-mae:0.508082\teval-mae:0.526681\n",
      "[50]\ttrain-mae:0.507616\teval-mae:0.526374\n",
      "[51]\ttrain-mae:0.507046\teval-mae:0.526084\n",
      "[52]\ttrain-mae:0.506473\teval-mae:0.525801\n",
      "[53]\ttrain-mae:0.505957\teval-mae:0.525597\n",
      "[54]\ttrain-mae:0.505426\teval-mae:0.525316\n",
      "[55]\ttrain-mae:0.504938\teval-mae:0.525091\n",
      "[56]\ttrain-mae:0.50447\teval-mae:0.524865\n",
      "[57]\ttrain-mae:0.503957\teval-mae:0.524586\n",
      "[58]\ttrain-mae:0.503388\teval-mae:0.524192\n",
      "[59]\ttrain-mae:0.503038\teval-mae:0.524028\n",
      "[60]\ttrain-mae:0.502554\teval-mae:0.523784\n",
      "[61]\ttrain-mae:0.50208\teval-mae:0.523614\n",
      "[62]\ttrain-mae:0.501578\teval-mae:0.523347\n",
      "[63]\ttrain-mae:0.501158\teval-mae:0.523108\n",
      "[64]\ttrain-mae:0.500761\teval-mae:0.522905\n",
      "[65]\ttrain-mae:0.500408\teval-mae:0.522803\n",
      "[66]\ttrain-mae:0.499971\teval-mae:0.52254\n",
      "[67]\ttrain-mae:0.499532\teval-mae:0.522405\n",
      "[68]\ttrain-mae:0.499121\teval-mae:0.522158\n",
      "[69]\ttrain-mae:0.49872\teval-mae:0.521964\n",
      "[70]\ttrain-mae:0.498394\teval-mae:0.521808\n",
      "[71]\ttrain-mae:0.497995\teval-mae:0.521638\n",
      "[72]\ttrain-mae:0.497616\teval-mae:0.521464\n",
      "[73]\ttrain-mae:0.497292\teval-mae:0.521342\n",
      "[74]\ttrain-mae:0.496948\teval-mae:0.521172\n",
      "[75]\ttrain-mae:0.496685\teval-mae:0.521045\n",
      "[76]\ttrain-mae:0.496335\teval-mae:0.520836\n",
      "[77]\ttrain-mae:0.496127\teval-mae:0.520757\n",
      "[78]\ttrain-mae:0.495845\teval-mae:0.520649\n",
      "[79]\ttrain-mae:0.495576\teval-mae:0.520531\n",
      "[80]\ttrain-mae:0.495272\teval-mae:0.52043\n",
      "[81]\ttrain-mae:0.495024\teval-mae:0.520364\n",
      "[82]\ttrain-mae:0.494646\teval-mae:0.520202\n",
      "[83]\ttrain-mae:0.494419\teval-mae:0.520167\n",
      "[84]\ttrain-mae:0.494122\teval-mae:0.520039\n",
      "[85]\ttrain-mae:0.493735\teval-mae:0.519867\n",
      "[86]\ttrain-mae:0.493506\teval-mae:0.519795\n",
      "[87]\ttrain-mae:0.493194\teval-mae:0.519747\n",
      "[88]\ttrain-mae:0.492872\teval-mae:0.519627\n",
      "[89]\ttrain-mae:0.492696\teval-mae:0.519563\n",
      "[90]\ttrain-mae:0.492545\teval-mae:0.519507\n",
      "[91]\ttrain-mae:0.492265\teval-mae:0.519437\n",
      "[92]\ttrain-mae:0.492001\teval-mae:0.51929\n",
      "[93]\ttrain-mae:0.491668\teval-mae:0.519179\n",
      "[94]\ttrain-mae:0.491451\teval-mae:0.519148\n",
      "[95]\ttrain-mae:0.49125\teval-mae:0.519019\n",
      "[96]\ttrain-mae:0.49107\teval-mae:0.518963\n",
      "[97]\ttrain-mae:0.490903\teval-mae:0.518929\n",
      "[98]\ttrain-mae:0.490529\teval-mae:0.518745\n",
      "[99]\ttrain-mae:0.49032\teval-mae:0.5187\n",
      "[100]\ttrain-mae:0.490107\teval-mae:0.518655\n",
      "[101]\ttrain-mae:0.489889\teval-mae:0.518562\n",
      "[102]\ttrain-mae:0.489603\teval-mae:0.518498\n",
      "[103]\ttrain-mae:0.489345\teval-mae:0.5185\n",
      "[104]\ttrain-mae:0.48917\teval-mae:0.518441\n",
      "[105]\ttrain-mae:0.488946\teval-mae:0.51841\n",
      "[106]\ttrain-mae:0.488784\teval-mae:0.518334\n",
      "[107]\ttrain-mae:0.488581\teval-mae:0.518277\n",
      "[108]\ttrain-mae:0.488371\teval-mae:0.518264\n",
      "[109]\ttrain-mae:0.48819\teval-mae:0.518226\n",
      "[110]\ttrain-mae:0.487853\teval-mae:0.518094\n",
      "[111]\ttrain-mae:0.487781\teval-mae:0.518086\n",
      "[112]\ttrain-mae:0.487514\teval-mae:0.518019\n",
      "[113]\ttrain-mae:0.487288\teval-mae:0.518001\n",
      "[114]\ttrain-mae:0.4871\teval-mae:0.51795\n",
      "[115]\ttrain-mae:0.486675\teval-mae:0.517828\n",
      "[116]\ttrain-mae:0.48645\teval-mae:0.517731\n",
      "[117]\ttrain-mae:0.486252\teval-mae:0.517695\n",
      "[118]\ttrain-mae:0.486029\teval-mae:0.517639\n",
      "[119]\ttrain-mae:0.485854\teval-mae:0.517567\n",
      "[120]\ttrain-mae:0.485704\teval-mae:0.517523\n",
      "[121]\ttrain-mae:0.485576\teval-mae:0.517485\n",
      "[122]\ttrain-mae:0.485263\teval-mae:0.517455\n",
      "[123]\ttrain-mae:0.485178\teval-mae:0.517449\n",
      "[124]\ttrain-mae:0.48503\teval-mae:0.517446\n",
      "[125]\ttrain-mae:0.48483\teval-mae:0.517405\n",
      "[126]\ttrain-mae:0.484522\teval-mae:0.517393\n",
      "[127]\ttrain-mae:0.484294\teval-mae:0.517319\n",
      "[128]\ttrain-mae:0.484132\teval-mae:0.517281\n",
      "[129]\ttrain-mae:0.483998\teval-mae:0.517283\n",
      "[130]\ttrain-mae:0.483711\teval-mae:0.517169\n",
      "[131]\ttrain-mae:0.483634\teval-mae:0.51715\n",
      "[132]\ttrain-mae:0.483491\teval-mae:0.517144\n",
      "[133]\ttrain-mae:0.483408\teval-mae:0.517138\n",
      "[134]\ttrain-mae:0.483251\teval-mae:0.517128\n",
      "[135]\ttrain-mae:0.483099\teval-mae:0.517103\n",
      "[136]\ttrain-mae:0.482874\teval-mae:0.517017\n",
      "[137]\ttrain-mae:0.482799\teval-mae:0.517001\n",
      "[138]\ttrain-mae:0.482636\teval-mae:0.516947\n",
      "[139]\ttrain-mae:0.48235\teval-mae:0.51688\n",
      "[140]\ttrain-mae:0.482016\teval-mae:0.516782\n",
      "[141]\ttrain-mae:0.481853\teval-mae:0.516792\n",
      "[142]\ttrain-mae:0.481729\teval-mae:0.51678\n",
      "[143]\ttrain-mae:0.481664\teval-mae:0.5168\n",
      "[144]\ttrain-mae:0.481432\teval-mae:0.516782\n",
      "[145]\ttrain-mae:0.481332\teval-mae:0.516754\n",
      "[146]\ttrain-mae:0.481046\teval-mae:0.516688\n",
      "[147]\ttrain-mae:0.480891\teval-mae:0.516661\n",
      "[148]\ttrain-mae:0.480773\teval-mae:0.516644\n",
      "[149]\ttrain-mae:0.480552\teval-mae:0.516629\n",
      "[150]\ttrain-mae:0.480447\teval-mae:0.51663\n",
      "[151]\ttrain-mae:0.480342\teval-mae:0.516641\n",
      "[152]\ttrain-mae:0.48017\teval-mae:0.516648\n",
      "[153]\ttrain-mae:0.480021\teval-mae:0.516664\n",
      "[154]\ttrain-mae:0.479825\teval-mae:0.516643\n",
      "[155]\ttrain-mae:0.479526\teval-mae:0.516532\n",
      "[156]\ttrain-mae:0.479465\teval-mae:0.51651\n",
      "[157]\ttrain-mae:0.47918\teval-mae:0.516523\n",
      "[158]\ttrain-mae:0.479143\teval-mae:0.516502\n",
      "[159]\ttrain-mae:0.478888\teval-mae:0.5164\n",
      "[160]\ttrain-mae:0.478661\teval-mae:0.516368\n",
      "[161]\ttrain-mae:0.478482\teval-mae:0.516371\n",
      "[162]\ttrain-mae:0.478331\teval-mae:0.516358\n",
      "[163]\ttrain-mae:0.478277\teval-mae:0.516345\n",
      "[164]\ttrain-mae:0.478062\teval-mae:0.516323\n",
      "[165]\ttrain-mae:0.477942\teval-mae:0.516328\n",
      "[166]\ttrain-mae:0.477785\teval-mae:0.516304\n",
      "[167]\ttrain-mae:0.477579\teval-mae:0.516275\n",
      "[168]\ttrain-mae:0.477457\teval-mae:0.516261\n",
      "[169]\ttrain-mae:0.477336\teval-mae:0.516288\n",
      "[170]\ttrain-mae:0.47722\teval-mae:0.516286\n",
      "[171]\ttrain-mae:0.477143\teval-mae:0.516274\n",
      "[172]\ttrain-mae:0.476911\teval-mae:0.516249\n",
      "[173]\ttrain-mae:0.476851\teval-mae:0.516242\n",
      "[174]\ttrain-mae:0.476668\teval-mae:0.516228\n",
      "[175]\ttrain-mae:0.476463\teval-mae:0.516222\n",
      "[176]\ttrain-mae:0.476266\teval-mae:0.516233\n",
      "[177]\ttrain-mae:0.476188\teval-mae:0.51623\n",
      "[178]\ttrain-mae:0.475915\teval-mae:0.516151\n",
      "[179]\ttrain-mae:0.47587\teval-mae:0.516139\n",
      "[180]\ttrain-mae:0.475734\teval-mae:0.516136\n",
      "[181]\ttrain-mae:0.475577\teval-mae:0.516093\n",
      "[182]\ttrain-mae:0.475534\teval-mae:0.516107\n",
      "[183]\ttrain-mae:0.47539\teval-mae:0.516086\n",
      "[184]\ttrain-mae:0.475288\teval-mae:0.516064\n",
      "[185]\ttrain-mae:0.475161\teval-mae:0.516085\n",
      "[186]\ttrain-mae:0.475004\teval-mae:0.516056\n",
      "[187]\ttrain-mae:0.474859\teval-mae:0.516045\n",
      "[188]\ttrain-mae:0.474511\teval-mae:0.515945\n",
      "[189]\ttrain-mae:0.474249\teval-mae:0.51592\n",
      "[190]\ttrain-mae:0.474122\teval-mae:0.515904\n",
      "[191]\ttrain-mae:0.473981\teval-mae:0.515903\n",
      "[192]\ttrain-mae:0.473853\teval-mae:0.515919\n",
      "[193]\ttrain-mae:0.473599\teval-mae:0.515829\n",
      "[194]\ttrain-mae:0.473406\teval-mae:0.515826\n",
      "[195]\ttrain-mae:0.473203\teval-mae:0.515799\n",
      "[196]\ttrain-mae:0.472957\teval-mae:0.515712\n",
      "[197]\ttrain-mae:0.472764\teval-mae:0.515634\n",
      "[198]\ttrain-mae:0.472568\teval-mae:0.515581\n",
      "[199]\ttrain-mae:0.472439\teval-mae:0.515574\n",
      "[200]\ttrain-mae:0.472307\teval-mae:0.515589\n",
      "[201]\ttrain-mae:0.472059\teval-mae:0.515578\n",
      "[202]\ttrain-mae:0.472013\teval-mae:0.515579\n",
      "[203]\ttrain-mae:0.471949\teval-mae:0.515578\n",
      "[204]\ttrain-mae:0.471813\teval-mae:0.515544\n",
      "[205]\ttrain-mae:0.471576\teval-mae:0.515527\n",
      "[206]\ttrain-mae:0.471487\teval-mae:0.515507\n",
      "[207]\ttrain-mae:0.471424\teval-mae:0.515506\n",
      "[208]\ttrain-mae:0.471372\teval-mae:0.515513\n",
      "[209]\ttrain-mae:0.471318\teval-mae:0.515508\n",
      "[210]\ttrain-mae:0.471176\teval-mae:0.515504\n",
      "[211]\ttrain-mae:0.470985\teval-mae:0.515497\n",
      "[212]\ttrain-mae:0.4709\teval-mae:0.515494\n",
      "[213]\ttrain-mae:0.470784\teval-mae:0.515499\n",
      "[214]\ttrain-mae:0.470621\teval-mae:0.515469\n",
      "[215]\ttrain-mae:0.470423\teval-mae:0.515481\n",
      "[216]\ttrain-mae:0.470341\teval-mae:0.515481\n",
      "[217]\ttrain-mae:0.470209\teval-mae:0.515459\n",
      "[218]\ttrain-mae:0.470019\teval-mae:0.515455\n",
      "[219]\ttrain-mae:0.469812\teval-mae:0.515402\n",
      "[220]\ttrain-mae:0.469533\teval-mae:0.515349\n",
      "[221]\ttrain-mae:0.469285\teval-mae:0.515312\n",
      "[222]\ttrain-mae:0.469112\teval-mae:0.51528\n",
      "[223]\ttrain-mae:0.468892\teval-mae:0.515275\n",
      "[224]\ttrain-mae:0.468745\teval-mae:0.515279\n",
      "[225]\ttrain-mae:0.468619\teval-mae:0.515275\n",
      "[226]\ttrain-mae:0.468522\teval-mae:0.515277\n",
      "[227]\ttrain-mae:0.468283\teval-mae:0.51525\n",
      "[228]\ttrain-mae:0.468148\teval-mae:0.515247\n",
      "[229]\ttrain-mae:0.468081\teval-mae:0.515233\n",
      "[230]\ttrain-mae:0.468011\teval-mae:0.515228\n",
      "[231]\ttrain-mae:0.467839\teval-mae:0.515273\n",
      "[232]\ttrain-mae:0.467759\teval-mae:0.515273\n",
      "[233]\ttrain-mae:0.467578\teval-mae:0.51526\n",
      "[234]\ttrain-mae:0.467383\teval-mae:0.515231\n",
      "[235]\ttrain-mae:0.46727\teval-mae:0.515228\n",
      "[236]\ttrain-mae:0.467102\teval-mae:0.515274\n",
      "[237]\ttrain-mae:0.466994\teval-mae:0.515281\n",
      "[238]\ttrain-mae:0.466917\teval-mae:0.515288\n",
      "[239]\ttrain-mae:0.466863\teval-mae:0.515285\n",
      "[240]\ttrain-mae:0.466666\teval-mae:0.515304\n",
      "[241]\ttrain-mae:0.466561\teval-mae:0.515306\n",
      "[242]\ttrain-mae:0.466404\teval-mae:0.515298\n",
      "[243]\ttrain-mae:0.466357\teval-mae:0.515297\n",
      "[244]\ttrain-mae:0.466262\teval-mae:0.515275\n",
      "[245]\ttrain-mae:0.466164\teval-mae:0.515284\n",
      "[246]\ttrain-mae:0.466058\teval-mae:0.515279\n",
      "[247]\ttrain-mae:0.465865\teval-mae:0.515282\n",
      "[248]\ttrain-mae:0.465636\teval-mae:0.51529\n",
      "[249]\ttrain-mae:0.465508\teval-mae:0.515286\n",
      "[250]\ttrain-mae:0.465434\teval-mae:0.515294\n",
      "[251]\ttrain-mae:0.465147\teval-mae:0.515241\n",
      "[252]\ttrain-mae:0.465031\teval-mae:0.515251\n",
      "[253]\ttrain-mae:0.46482\teval-mae:0.515272\n",
      "[254]\ttrain-mae:0.464684\teval-mae:0.515256\n",
      "[255]\ttrain-mae:0.46448\teval-mae:0.515269\n",
      "[256]\ttrain-mae:0.464433\teval-mae:0.515273\n",
      "[257]\ttrain-mae:0.464388\teval-mae:0.515273\n",
      "[258]\ttrain-mae:0.46432\teval-mae:0.515288\n",
      "[259]\ttrain-mae:0.464149\teval-mae:0.515291\n",
      "[260]\ttrain-mae:0.46407\teval-mae:0.515302\n",
      "[261]\ttrain-mae:0.463923\teval-mae:0.515296\n",
      "[262]\ttrain-mae:0.463679\teval-mae:0.515315\n",
      "[263]\ttrain-mae:0.463533\teval-mae:0.515289\n",
      "[264]\ttrain-mae:0.463283\teval-mae:0.515279\n",
      "[265]\ttrain-mae:0.463073\teval-mae:0.515312\n",
      "[266]\ttrain-mae:0.463032\teval-mae:0.51532\n",
      "[267]\ttrain-mae:0.462946\teval-mae:0.515335\n",
      "[268]\ttrain-mae:0.46288\teval-mae:0.515332\n",
      "[269]\ttrain-mae:0.462846\teval-mae:0.515334\n",
      "[270]\ttrain-mae:0.462754\teval-mae:0.515341\n",
      "[271]\ttrain-mae:0.462647\teval-mae:0.515362\n",
      "[272]\ttrain-mae:0.46247\teval-mae:0.515386\n",
      "[273]\ttrain-mae:0.462221\teval-mae:0.515413\n",
      "[274]\ttrain-mae:0.462094\teval-mae:0.515393\n",
      "[275]\ttrain-mae:0.461973\teval-mae:0.515393\n",
      "[276]\ttrain-mae:0.461919\teval-mae:0.515391\n",
      "[277]\ttrain-mae:0.461822\teval-mae:0.515387\n",
      "[278]\ttrain-mae:0.461513\teval-mae:0.515312\n",
      "[279]\ttrain-mae:0.461377\teval-mae:0.515297\n",
      "[280]\ttrain-mae:0.46124\teval-mae:0.51527\n",
      "[281]\ttrain-mae:0.461123\teval-mae:0.515283\n",
      "[282]\ttrain-mae:0.460902\teval-mae:0.51528\n",
      "[283]\ttrain-mae:0.460778\teval-mae:0.515279\n",
      "[284]\ttrain-mae:0.460708\teval-mae:0.515263\n",
      "[285]\ttrain-mae:0.46055\teval-mae:0.515293\n",
      "[286]\ttrain-mae:0.460482\teval-mae:0.51531\n",
      "[287]\ttrain-mae:0.460263\teval-mae:0.515298\n",
      "[288]\ttrain-mae:0.460226\teval-mae:0.515298\n",
      "[289]\ttrain-mae:0.460199\teval-mae:0.515295\n",
      "[290]\ttrain-mae:0.460066\teval-mae:0.515318\n",
      "[291]\ttrain-mae:0.459907\teval-mae:0.515319\n",
      "[292]\ttrain-mae:0.459699\teval-mae:0.515311\n",
      "[293]\ttrain-mae:0.459511\teval-mae:0.5153\n",
      "[294]\ttrain-mae:0.459288\teval-mae:0.515262\n",
      "[295]\ttrain-mae:0.459094\teval-mae:0.515279\n",
      "[296]\ttrain-mae:0.458946\teval-mae:0.515272\n",
      "[297]\ttrain-mae:0.458906\teval-mae:0.51527\n",
      "[298]\ttrain-mae:0.45878\teval-mae:0.515277\n",
      "[299]\ttrain-mae:0.458633\teval-mae:0.515288\n",
      "('mae for all train', 1, 1054.5341309753742)\n",
      "[0]\ttrain-mae:0.863882\teval-mae:0.862217\n",
      "[1]\ttrain-mae:0.818375\teval-mae:0.81734\n",
      "[2]\ttrain-mae:0.780003\teval-mae:0.779532\n",
      "[3]\ttrain-mae:0.745704\teval-mae:0.745783\n",
      "[4]\ttrain-mae:0.717185\teval-mae:0.717965\n",
      "[5]\ttrain-mae:0.692799\teval-mae:0.694165\n",
      "[6]\ttrain-mae:0.672022\teval-mae:0.673856\n",
      "[7]\ttrain-mae:0.65355\teval-mae:0.655816\n",
      "[8]\ttrain-mae:0.637908\teval-mae:0.640655\n",
      "[9]\ttrain-mae:0.624022\teval-mae:0.627051\n",
      "[10]\ttrain-mae:0.612425\teval-mae:0.615721\n",
      "[11]\ttrain-mae:0.601785\teval-mae:0.605435\n",
      "[12]\ttrain-mae:0.592902\teval-mae:0.596936\n",
      "[13]\ttrain-mae:0.585131\teval-mae:0.58964\n",
      "[14]\ttrain-mae:0.578401\teval-mae:0.583279\n",
      "[15]\ttrain-mae:0.572327\teval-mae:0.57766\n",
      "[16]\ttrain-mae:0.567083\teval-mae:0.572823\n",
      "[17]\ttrain-mae:0.562434\teval-mae:0.568566\n",
      "[18]\ttrain-mae:0.558038\teval-mae:0.564461\n",
      "[19]\ttrain-mae:0.554203\teval-mae:0.560859\n",
      "[20]\ttrain-mae:0.550723\teval-mae:0.557721\n",
      "[21]\ttrain-mae:0.547444\teval-mae:0.55481\n",
      "[22]\ttrain-mae:0.54457\teval-mae:0.552317\n",
      "[23]\ttrain-mae:0.542178\teval-mae:0.550197\n",
      "[24]\ttrain-mae:0.539732\teval-mae:0.548027\n",
      "[25]\ttrain-mae:0.537714\teval-mae:0.546387\n",
      "[26]\ttrain-mae:0.53568\teval-mae:0.544671\n",
      "[27]\ttrain-mae:0.53377\teval-mae:0.54318\n",
      "[28]\ttrain-mae:0.531996\teval-mae:0.541747\n",
      "[29]\ttrain-mae:0.530482\teval-mae:0.540492\n",
      "[30]\ttrain-mae:0.528898\teval-mae:0.539125\n",
      "[31]\ttrain-mae:0.527549\teval-mae:0.538051\n",
      "[32]\ttrain-mae:0.526073\teval-mae:0.53694\n",
      "[33]\ttrain-mae:0.524805\teval-mae:0.536017\n",
      "[34]\ttrain-mae:0.523721\teval-mae:0.535184\n",
      "[35]\ttrain-mae:0.522625\teval-mae:0.534423\n",
      "[36]\ttrain-mae:0.521414\teval-mae:0.533501\n",
      "[37]\ttrain-mae:0.520074\teval-mae:0.532353\n",
      "[38]\ttrain-mae:0.51907\teval-mae:0.5315\n",
      "[39]\ttrain-mae:0.518158\teval-mae:0.5309\n",
      "[40]\ttrain-mae:0.517221\teval-mae:0.530238\n",
      "[41]\ttrain-mae:0.516251\teval-mae:0.529593\n",
      "[42]\ttrain-mae:0.515244\teval-mae:0.528715\n",
      "[43]\ttrain-mae:0.514412\teval-mae:0.528088\n",
      "[44]\ttrain-mae:0.513597\teval-mae:0.527535\n",
      "[45]\ttrain-mae:0.512981\teval-mae:0.527177\n",
      "[46]\ttrain-mae:0.512369\teval-mae:0.526804\n",
      "[47]\ttrain-mae:0.511646\teval-mae:0.526414\n",
      "[48]\ttrain-mae:0.510969\teval-mae:0.525962\n",
      "[49]\ttrain-mae:0.510406\teval-mae:0.52563\n",
      "[50]\ttrain-mae:0.509808\teval-mae:0.525293\n",
      "[51]\ttrain-mae:0.509196\teval-mae:0.524885\n",
      "[52]\ttrain-mae:0.508658\teval-mae:0.524603\n",
      "[53]\ttrain-mae:0.507981\teval-mae:0.524209\n",
      "[54]\ttrain-mae:0.507262\teval-mae:0.523767\n",
      "[55]\ttrain-mae:0.506815\teval-mae:0.523563\n",
      "[56]\ttrain-mae:0.506421\teval-mae:0.523315\n",
      "[57]\ttrain-mae:0.505831\teval-mae:0.522958\n",
      "[58]\ttrain-mae:0.505349\teval-mae:0.522695\n",
      "[59]\ttrain-mae:0.504858\teval-mae:0.522391\n",
      "[60]\ttrain-mae:0.504354\teval-mae:0.522173\n",
      "[61]\ttrain-mae:0.50399\teval-mae:0.522042\n",
      "[62]\ttrain-mae:0.50317\teval-mae:0.521526\n",
      "[63]\ttrain-mae:0.502728\teval-mae:0.521336\n",
      "[64]\ttrain-mae:0.502456\teval-mae:0.521247\n",
      "[65]\ttrain-mae:0.502102\teval-mae:0.521158\n",
      "[66]\ttrain-mae:0.501511\teval-mae:0.520767\n",
      "[67]\ttrain-mae:0.501079\teval-mae:0.520552\n",
      "[68]\ttrain-mae:0.500698\teval-mae:0.52041\n",
      "[69]\ttrain-mae:0.500245\teval-mae:0.520197\n",
      "[70]\ttrain-mae:0.499909\teval-mae:0.520056\n",
      "[71]\ttrain-mae:0.499621\teval-mae:0.519937\n",
      "[72]\ttrain-mae:0.499204\teval-mae:0.519816\n",
      "[73]\ttrain-mae:0.498719\teval-mae:0.519634\n",
      "[74]\ttrain-mae:0.498398\teval-mae:0.519444\n",
      "[75]\ttrain-mae:0.498159\teval-mae:0.519341\n",
      "[76]\ttrain-mae:0.497866\teval-mae:0.519284\n",
      "[77]\ttrain-mae:0.497525\teval-mae:0.519147\n",
      "[78]\ttrain-mae:0.497171\teval-mae:0.519049\n",
      "[79]\ttrain-mae:0.496957\teval-mae:0.518981\n",
      "[80]\ttrain-mae:0.496736\teval-mae:0.518871\n",
      "[81]\ttrain-mae:0.49635\teval-mae:0.518654\n",
      "[82]\ttrain-mae:0.495996\teval-mae:0.518519\n",
      "[83]\ttrain-mae:0.495742\teval-mae:0.518464\n",
      "[84]\ttrain-mae:0.495446\teval-mae:0.518336\n",
      "[85]\ttrain-mae:0.495028\teval-mae:0.518209\n",
      "[86]\ttrain-mae:0.494754\teval-mae:0.518125\n",
      "[87]\ttrain-mae:0.494419\teval-mae:0.51804\n",
      "[88]\ttrain-mae:0.493973\teval-mae:0.517821\n",
      "[89]\ttrain-mae:0.493666\teval-mae:0.517657\n",
      "[90]\ttrain-mae:0.49336\teval-mae:0.517502\n",
      "[91]\ttrain-mae:0.493091\teval-mae:0.517436\n",
      "[92]\ttrain-mae:0.492645\teval-mae:0.51727\n",
      "[93]\ttrain-mae:0.492374\teval-mae:0.517121\n",
      "[94]\ttrain-mae:0.492157\teval-mae:0.517111\n",
      "[95]\ttrain-mae:0.491968\teval-mae:0.517054\n",
      "[96]\ttrain-mae:0.491657\teval-mae:0.51693\n",
      "[97]\ttrain-mae:0.49137\teval-mae:0.516904\n",
      "[98]\ttrain-mae:0.49109\teval-mae:0.516803\n",
      "[99]\ttrain-mae:0.490894\teval-mae:0.516755\n",
      "[100]\ttrain-mae:0.49053\teval-mae:0.516646\n",
      "[101]\ttrain-mae:0.490328\teval-mae:0.516548\n",
      "[102]\ttrain-mae:0.489992\teval-mae:0.516451\n",
      "[103]\ttrain-mae:0.489817\teval-mae:0.516388\n",
      "[104]\ttrain-mae:0.489407\teval-mae:0.516167\n",
      "[105]\ttrain-mae:0.489138\teval-mae:0.516078\n",
      "[106]\ttrain-mae:0.488932\teval-mae:0.516032\n",
      "[107]\ttrain-mae:0.488594\teval-mae:0.51594\n",
      "[108]\ttrain-mae:0.488289\teval-mae:0.515864\n",
      "[109]\ttrain-mae:0.487998\teval-mae:0.515779\n",
      "[110]\ttrain-mae:0.487847\teval-mae:0.515762\n",
      "[111]\ttrain-mae:0.487681\teval-mae:0.515699\n",
      "[112]\ttrain-mae:0.48752\teval-mae:0.515682\n",
      "[113]\ttrain-mae:0.487296\teval-mae:0.515579\n",
      "[114]\ttrain-mae:0.48706\teval-mae:0.515537\n",
      "[115]\ttrain-mae:0.486885\teval-mae:0.515522\n",
      "[116]\ttrain-mae:0.48675\teval-mae:0.515505\n",
      "[117]\ttrain-mae:0.486547\teval-mae:0.515506\n",
      "[118]\ttrain-mae:0.486295\teval-mae:0.515468\n",
      "[119]\ttrain-mae:0.486135\teval-mae:0.515444\n",
      "[120]\ttrain-mae:0.48587\teval-mae:0.515359\n",
      "[121]\ttrain-mae:0.485647\teval-mae:0.515287\n",
      "[122]\ttrain-mae:0.485378\teval-mae:0.515191\n",
      "[123]\ttrain-mae:0.485072\teval-mae:0.515108\n",
      "[124]\ttrain-mae:0.484888\teval-mae:0.515094\n",
      "[125]\ttrain-mae:0.484609\teval-mae:0.515087\n",
      "[126]\ttrain-mae:0.484415\teval-mae:0.51505\n",
      "[127]\ttrain-mae:0.484141\teval-mae:0.514944\n",
      "[128]\ttrain-mae:0.483991\teval-mae:0.514903\n",
      "[129]\ttrain-mae:0.48371\teval-mae:0.514853\n",
      "[130]\ttrain-mae:0.483458\teval-mae:0.514793\n",
      "[131]\ttrain-mae:0.483146\teval-mae:0.514733\n",
      "[132]\ttrain-mae:0.482932\teval-mae:0.514725\n",
      "[133]\ttrain-mae:0.482574\teval-mae:0.514585\n",
      "[134]\ttrain-mae:0.482387\teval-mae:0.514589\n",
      "[135]\ttrain-mae:0.482123\teval-mae:0.514551\n",
      "[136]\ttrain-mae:0.481993\teval-mae:0.514525\n",
      "[137]\ttrain-mae:0.481784\teval-mae:0.514472\n",
      "[138]\ttrain-mae:0.481641\teval-mae:0.514447\n",
      "[139]\ttrain-mae:0.48146\teval-mae:0.514403\n",
      "[140]\ttrain-mae:0.4812\teval-mae:0.514312\n",
      "[141]\ttrain-mae:0.480991\teval-mae:0.514295\n",
      "[142]\ttrain-mae:0.480812\teval-mae:0.51428\n",
      "[143]\ttrain-mae:0.480607\teval-mae:0.514266\n",
      "[144]\ttrain-mae:0.480444\teval-mae:0.51425\n",
      "[145]\ttrain-mae:0.480331\teval-mae:0.514248\n",
      "[146]\ttrain-mae:0.480199\teval-mae:0.514254\n",
      "[147]\ttrain-mae:0.479969\teval-mae:0.514191\n",
      "[148]\ttrain-mae:0.479928\teval-mae:0.514162\n",
      "[149]\ttrain-mae:0.479754\teval-mae:0.514156\n",
      "[150]\ttrain-mae:0.479551\teval-mae:0.514109\n",
      "[151]\ttrain-mae:0.479464\teval-mae:0.514114\n",
      "[152]\ttrain-mae:0.479302\teval-mae:0.514085\n",
      "[153]\ttrain-mae:0.479212\teval-mae:0.514073\n",
      "[154]\ttrain-mae:0.479104\teval-mae:0.514059\n",
      "[155]\ttrain-mae:0.478957\teval-mae:0.514072\n",
      "[156]\ttrain-mae:0.478816\teval-mae:0.514031\n",
      "[157]\ttrain-mae:0.478767\teval-mae:0.514034\n",
      "[158]\ttrain-mae:0.478563\teval-mae:0.513985\n",
      "[159]\ttrain-mae:0.478401\teval-mae:0.513974\n",
      "[160]\ttrain-mae:0.478206\teval-mae:0.513939\n",
      "[161]\ttrain-mae:0.478057\teval-mae:0.513958\n",
      "[162]\ttrain-mae:0.477977\teval-mae:0.513946\n",
      "[163]\ttrain-mae:0.477864\teval-mae:0.513952\n",
      "[164]\ttrain-mae:0.477721\teval-mae:0.513921\n",
      "[165]\ttrain-mae:0.477511\teval-mae:0.513871\n",
      "[166]\ttrain-mae:0.477202\teval-mae:0.513758\n",
      "[167]\ttrain-mae:0.476934\teval-mae:0.513687\n",
      "[168]\ttrain-mae:0.476799\teval-mae:0.513691\n",
      "[169]\ttrain-mae:0.476667\teval-mae:0.513665\n",
      "[170]\ttrain-mae:0.476513\teval-mae:0.51365\n",
      "[171]\ttrain-mae:0.476296\teval-mae:0.513639\n",
      "[172]\ttrain-mae:0.47614\teval-mae:0.513661\n",
      "[173]\ttrain-mae:0.475983\teval-mae:0.513621\n",
      "[174]\ttrain-mae:0.475827\teval-mae:0.513589\n",
      "[175]\ttrain-mae:0.475757\teval-mae:0.513591\n",
      "[176]\ttrain-mae:0.47562\teval-mae:0.51358\n",
      "[177]\ttrain-mae:0.47541\teval-mae:0.513544\n",
      "[178]\ttrain-mae:0.475205\teval-mae:0.513511\n",
      "[179]\ttrain-mae:0.47509\teval-mae:0.513504\n",
      "[180]\ttrain-mae:0.474837\teval-mae:0.51348\n",
      "[181]\ttrain-mae:0.47471\teval-mae:0.51346\n",
      "[182]\ttrain-mae:0.474546\teval-mae:0.513469\n",
      "[183]\ttrain-mae:0.474444\teval-mae:0.513454\n",
      "[184]\ttrain-mae:0.474225\teval-mae:0.513441\n",
      "[185]\ttrain-mae:0.474118\teval-mae:0.513425\n",
      "[186]\ttrain-mae:0.47396\teval-mae:0.513385\n",
      "[187]\ttrain-mae:0.473716\teval-mae:0.513357\n",
      "[188]\ttrain-mae:0.47354\teval-mae:0.513333\n",
      "[189]\ttrain-mae:0.473379\teval-mae:0.51332\n",
      "[190]\ttrain-mae:0.473238\teval-mae:0.513311\n",
      "[191]\ttrain-mae:0.473079\teval-mae:0.513298\n",
      "[192]\ttrain-mae:0.472908\teval-mae:0.513316\n",
      "[193]\ttrain-mae:0.472797\teval-mae:0.513289\n",
      "[194]\ttrain-mae:0.472429\teval-mae:0.513274\n",
      "[195]\ttrain-mae:0.472249\teval-mae:0.513245\n",
      "[196]\ttrain-mae:0.472125\teval-mae:0.513227\n",
      "[197]\ttrain-mae:0.472029\teval-mae:0.51322\n",
      "[198]\ttrain-mae:0.471825\teval-mae:0.513178\n",
      "[199]\ttrain-mae:0.471684\teval-mae:0.513149\n",
      "[200]\ttrain-mae:0.471643\teval-mae:0.513159\n",
      "[201]\ttrain-mae:0.471439\teval-mae:0.513127\n",
      "[202]\ttrain-mae:0.471299\teval-mae:0.513167\n",
      "[203]\ttrain-mae:0.471189\teval-mae:0.513148\n",
      "[204]\ttrain-mae:0.471123\teval-mae:0.513162\n",
      "[205]\ttrain-mae:0.470952\teval-mae:0.513153\n",
      "[206]\ttrain-mae:0.470732\teval-mae:0.513111\n",
      "[207]\ttrain-mae:0.470558\teval-mae:0.513068\n",
      "[208]\ttrain-mae:0.470437\teval-mae:0.513061\n",
      "[209]\ttrain-mae:0.470229\teval-mae:0.513021\n",
      "[210]\ttrain-mae:0.470165\teval-mae:0.513018\n",
      "[211]\ttrain-mae:0.470009\teval-mae:0.513032\n",
      "[212]\ttrain-mae:0.46977\teval-mae:0.513022\n",
      "[213]\ttrain-mae:0.469621\teval-mae:0.513039\n",
      "[214]\ttrain-mae:0.469514\teval-mae:0.513036\n",
      "[215]\ttrain-mae:0.469373\teval-mae:0.513037\n",
      "[216]\ttrain-mae:0.469329\teval-mae:0.513039\n",
      "[217]\ttrain-mae:0.469265\teval-mae:0.513064\n",
      "[218]\ttrain-mae:0.469066\teval-mae:0.513056\n",
      "[219]\ttrain-mae:0.468936\teval-mae:0.513032\n",
      "[220]\ttrain-mae:0.468869\teval-mae:0.513024\n",
      "[221]\ttrain-mae:0.468815\teval-mae:0.513033\n",
      "[222]\ttrain-mae:0.468558\teval-mae:0.512982\n",
      "[223]\ttrain-mae:0.468371\teval-mae:0.512976\n",
      "[224]\ttrain-mae:0.468127\teval-mae:0.512948\n",
      "[225]\ttrain-mae:0.467959\teval-mae:0.512971\n",
      "[226]\ttrain-mae:0.467846\teval-mae:0.51296\n",
      "[227]\ttrain-mae:0.46777\teval-mae:0.512973\n",
      "[228]\ttrain-mae:0.467718\teval-mae:0.512955\n",
      "[229]\ttrain-mae:0.467666\teval-mae:0.512959\n",
      "[230]\ttrain-mae:0.46751\teval-mae:0.512989\n",
      "[231]\ttrain-mae:0.467373\teval-mae:0.512978\n",
      "[232]\ttrain-mae:0.467232\teval-mae:0.513005\n",
      "[233]\ttrain-mae:0.467091\teval-mae:0.512991\n",
      "[234]\ttrain-mae:0.466803\teval-mae:0.512963\n",
      "[235]\ttrain-mae:0.466754\teval-mae:0.512954\n",
      "[236]\ttrain-mae:0.466701\teval-mae:0.512957\n",
      "[237]\ttrain-mae:0.466679\teval-mae:0.512955\n",
      "[238]\ttrain-mae:0.466529\teval-mae:0.512939\n",
      "[239]\ttrain-mae:0.466359\teval-mae:0.512947\n",
      "[240]\ttrain-mae:0.466185\teval-mae:0.512924\n",
      "[241]\ttrain-mae:0.466143\teval-mae:0.512934\n",
      "[242]\ttrain-mae:0.466097\teval-mae:0.512924\n",
      "[243]\ttrain-mae:0.465975\teval-mae:0.51289\n",
      "[244]\ttrain-mae:0.465903\teval-mae:0.512882\n",
      "[245]\ttrain-mae:0.465698\teval-mae:0.512864\n",
      "[246]\ttrain-mae:0.46559\teval-mae:0.512862\n",
      "[247]\ttrain-mae:0.465407\teval-mae:0.512836\n",
      "[248]\ttrain-mae:0.465258\teval-mae:0.512834\n",
      "[249]\ttrain-mae:0.465019\teval-mae:0.512841\n",
      "[250]\ttrain-mae:0.464962\teval-mae:0.512851\n",
      "[251]\ttrain-mae:0.464851\teval-mae:0.512871\n",
      "[252]\ttrain-mae:0.464692\teval-mae:0.512856\n",
      "[253]\ttrain-mae:0.464456\teval-mae:0.51283\n",
      "[254]\ttrain-mae:0.464231\teval-mae:0.512771\n",
      "[255]\ttrain-mae:0.464058\teval-mae:0.512775\n",
      "[256]\ttrain-mae:0.463881\teval-mae:0.512786\n",
      "[257]\ttrain-mae:0.463741\teval-mae:0.512787\n",
      "[258]\ttrain-mae:0.463523\teval-mae:0.512735\n",
      "[259]\ttrain-mae:0.463356\teval-mae:0.512707\n",
      "[260]\ttrain-mae:0.463204\teval-mae:0.512707\n",
      "[261]\ttrain-mae:0.463134\teval-mae:0.5127\n",
      "[262]\ttrain-mae:0.46301\teval-mae:0.512722\n",
      "[263]\ttrain-mae:0.462959\teval-mae:0.512736\n",
      "[264]\ttrain-mae:0.462899\teval-mae:0.512729\n",
      "[265]\ttrain-mae:0.462669\teval-mae:0.512722\n",
      "[266]\ttrain-mae:0.462618\teval-mae:0.512714\n",
      "[267]\ttrain-mae:0.462399\teval-mae:0.512693\n",
      "[268]\ttrain-mae:0.462288\teval-mae:0.512696\n",
      "[269]\ttrain-mae:0.462132\teval-mae:0.512681\n",
      "[270]\ttrain-mae:0.461916\teval-mae:0.512633\n",
      "[271]\ttrain-mae:0.461807\teval-mae:0.512639\n",
      "[272]\ttrain-mae:0.461663\teval-mae:0.512645\n",
      "[273]\ttrain-mae:0.461493\teval-mae:0.512646\n",
      "[274]\ttrain-mae:0.461336\teval-mae:0.512652\n",
      "[275]\ttrain-mae:0.461261\teval-mae:0.512637\n",
      "[276]\ttrain-mae:0.461139\teval-mae:0.512626\n",
      "[277]\ttrain-mae:0.460943\teval-mae:0.512652\n",
      "[278]\ttrain-mae:0.460816\teval-mae:0.512658\n",
      "[279]\ttrain-mae:0.460676\teval-mae:0.512695\n",
      "[280]\ttrain-mae:0.460597\teval-mae:0.512691\n",
      "[281]\ttrain-mae:0.460432\teval-mae:0.512688\n",
      "[282]\ttrain-mae:0.460219\teval-mae:0.512691\n",
      "[283]\ttrain-mae:0.459993\teval-mae:0.512688\n",
      "[284]\ttrain-mae:0.45996\teval-mae:0.512707\n",
      "[285]\ttrain-mae:0.459834\teval-mae:0.512697\n",
      "[286]\ttrain-mae:0.459732\teval-mae:0.512707\n",
      "[287]\ttrain-mae:0.459639\teval-mae:0.512697\n",
      "[288]\ttrain-mae:0.459432\teval-mae:0.512673\n",
      "[289]\ttrain-mae:0.459364\teval-mae:0.512683\n",
      "[290]\ttrain-mae:0.459342\teval-mae:0.512689\n",
      "[291]\ttrain-mae:0.459303\teval-mae:0.512683\n",
      "[292]\ttrain-mae:0.459121\teval-mae:0.512681\n",
      "[293]\ttrain-mae:0.458956\teval-mae:0.512686\n",
      "[294]\ttrain-mae:0.458872\teval-mae:0.512691\n",
      "[295]\ttrain-mae:0.458635\teval-mae:0.512666\n",
      "[296]\ttrain-mae:0.458444\teval-mae:0.512683\n",
      "[297]\ttrain-mae:0.458306\teval-mae:0.512652\n",
      "[298]\ttrain-mae:0.45825\teval-mae:0.512654\n",
      "[299]\ttrain-mae:0.458104\teval-mae:0.51266\n",
      "('mae for all train', 2, 1050.4624243610017)\n"
     ]
    }
   ],
   "source": [
    "folds = 3\n",
    "\n",
    "pred_test = 0.\n",
    "pred_train = 0.\n",
    "restored_pred_train = 0.\n",
    "restored_pred_test = 0.\n",
    "\n",
    "kf = KFold(n_splits=folds)\n",
    "kf.split(train)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "    train_pd_ind = train.index[train_index]\n",
    "    test_pd_ind = train.index[test_index]\n",
    "    train_part, test_part = train.ix[train_pd_ind], train.ix[test_pd_ind]\n",
    "    \n",
    "    dtrain_part = xgb.DMatrix(train_part.drop(\"loss\", 1), response[train_pd_ind])\n",
    "    dtest_part = xgb.DMatrix(test_part.drop(\"loss\", 1), response[test_pd_ind])\n",
    "    params['seed'] = i * 5 + 100\n",
    "    clf = xgb.train(params, dtrain_part, num_boost_round=300,\n",
    "                    evals=[(dtrain_part, \"train\"), (dtest_part, \"eval\")])\n",
    "    \n",
    "    this_pred_train = clf.predict(dtrain, ntree_limit=clf.best_ntree_limit)\n",
    "    print(\"mae for all train\",i, mean_absolute_error(train.loss, restore_pred(this_pred_train)))\n",
    "    \n",
    "    pred_train += this_pred_train\n",
    "    restored_pred_train += restore_pred(this_pred_train)\n",
    "    \n",
    "    this_pred_test = clf.predict(dtest, ntree_limit=clf.best_ntree_limit)\n",
    "    pred_test += this_pred_test\n",
    "    restored_pred_test += restore_pred(this_pred_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mae final restore after', 1043.7366157892552)\n",
      "('mae final restore before', 1042.7750201282063)\n"
     ]
    }
   ],
   "source": [
    "print(\"mae final restore after\", mean_absolute_error(train.loss, restore_pred(pred_train / folds)))\n",
    "print(\"mae final restore before\", mean_absolute_error(train.loss, restored_pred_train / folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "result = pd.DataFrame({\"id\": test.index, \"loss\": restored_pred_test / folds})\n",
    "result.to_csv(\"result_restored_before{:%Y%m%d%H}.csv\".format(datetime.datetime.now()), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using XGBRegressor and important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params_reg = dict(params)\n",
    "params_reg.pop(\"eta\")\n",
    "params_reg.pop('eval_metric')\n",
    "params_reg.pop('lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = XGBRegressor(**params_reg)\n",
    "reg.fit(train.drop(\"loss\", 1), train.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_predprob = reg.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg_booster = reg.booster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figsize(18, 5)\n",
    "feat_imp = pd.Series(reg.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "important_features = list(feat_imp[feat_imp > 4].index)\n",
    "print(\"important features:\", important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain_imp = xgb.DMatrix(train[important_features], train.loss)\n",
    "cvresult = xgb.cv(params, dtrain_imp, nfold=4, num_boost_round=50)\n",
    "print(cvresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params2 = {'base_score': 0.1, 'colsample_bytree': 0.9,\n",
    " 'eta': 0.3,\n",
    " 'eval_metric': 'mae',\n",
    " 'max_depth': 7,\n",
    " 'min_child_weight': 3,\n",
    " 'n_estimators': 10,\n",
    " 'objective': 'reg:linear',\n",
    " 'seed': 1,\n",
    " 'silent': True}\n",
    "regb = xgb.train(params2, dtrain_imp, num_boost_round=50, evals=[(dtrain_imp, \"train\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator=reg, \n",
    " param_grid = param_test1, scoring='neg_mean_squared_error',n_jobs=4, iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsearch1.fit(train.drop(\"loss\", 1), train.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
