{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import skew, boxcox\n",
    "from IPython.core.pylabtools import figsize\n",
    "import xgboost as xgb\n",
    "%matplotlib inline\n",
    "\n",
    "from os import path\n",
    "to_filename = lambda name: path.join(\"..\", \"data\", \"allstate\", name +\".csv\")\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: train (188318, 131), test (125546, 130)\n",
      "   cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 cat10   ...        cont6  \\\n",
      "id                                                      ...                \n",
      "1     A    B    A    B    A    A    A    A    B     A   ...     0.718367   \n",
      "2     A    B    A    A    A    A    A    A    B     B   ...     0.438917   \n",
      "\n",
      "       cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
      "id                                                                      \n",
      "1   0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
      "2   0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
      "\n",
      "      cont14     loss  \n",
      "id                     \n",
      "1   0.714843  2213.18  \n",
      "2   0.304496  1283.60  \n",
      "\n",
      "[2 rows x 131 columns]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(to_filename(\"train\"), index_col=0)\n",
    "test = pd.read_csv(to_filename(\"test\"), index_col=0)\n",
    "print(\"shape: train {}, test {}\".format(train.shape, test.shape))\n",
    "print(train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = np.log(train.loss)\n",
    "\n",
    "mean_resp = np.mean(response)\n",
    "std_resp = np.std(response)\n",
    "response = (response - mean_resp) / std_resp\n",
    "\n",
    "\n",
    "def restore_pred1(y):\n",
    "    return np.exp(y)\n",
    "\n",
    "def restore_pred(y):\n",
    "    return np.exp(y * std_resp + mean_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Categorical columns:', ['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cat19', 'cat20', 'cat21', 'cat22', 'cat23', 'cat24', 'cat25', 'cat26', 'cat27', 'cat28', 'cat29', 'cat30', 'cat31', 'cat32', 'cat33', 'cat34', 'cat35', 'cat36', 'cat37', 'cat38', 'cat39', 'cat40', 'cat41', 'cat42', 'cat43', 'cat44', 'cat45', 'cat46', 'cat47', 'cat48', 'cat49', 'cat50', 'cat51', 'cat52', 'cat53', 'cat54', 'cat55', 'cat56', 'cat57', 'cat58', 'cat59', 'cat60', 'cat61', 'cat62', 'cat63', 'cat64', 'cat65', 'cat66', 'cat67', 'cat68', 'cat69', 'cat70', 'cat71', 'cat72', 'cat73', 'cat74', 'cat75', 'cat76', 'cat77', 'cat78', 'cat79', 'cat80', 'cat81', 'cat82', 'cat83', 'cat84', 'cat85', 'cat86', 'cat87', 'cat88', 'cat89', 'cat90', 'cat91', 'cat92', 'cat93', 'cat94', 'cat95', 'cat96', 'cat97', 'cat98', 'cat99', 'cat100', 'cat101', 'cat102', 'cat103', 'cat104', 'cat105', 'cat106', 'cat107', 'cat108', 'cat109', 'cat110', 'cat111', 'cat112', 'cat113', 'cat114', 'cat115', 'cat116'])\n"
     ]
    }
   ],
   "source": [
    "cat_features = [col for col in train.columns if col.startswith(\"cat\")]\n",
    "print(\"Categorical columns:\", cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorical features preprocessing\n",
    "# Method 1: Encoding categorical features into int\n",
    "for col in cat_features:\n",
    "    encd = preprocessing.LabelEncoder()\n",
    "    encd.fit(train[col].value_counts().index.union(test[col].value_counts().index))\n",
    "    train[col] = encd.transform(train[col])\n",
    "    test[col] = encd.transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method 2: Using cardinal features for categorical features\n",
    "col = cat_features[0]\n",
    "test_col = train[col][:10].copy()\n",
    "for col in cat_features:\n",
    "    key_map = response.groupby(train[col]).mean().to_dict()\n",
    "    train[col] = train[col].replace(key_map)\n",
    "    for k in set(test[col].value_counts().index).difference(key_map.keys()):\n",
    "        key_map[k] = np.NAN\n",
    "    test[col] = test[col].replace(key_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Numerical columns:', ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14'])\n"
     ]
    }
   ],
   "source": [
    "# preprocess numerical features\n",
    "num_features = [col for col in train.columns if col.startswith(\"cont\")]\n",
    "print(\"Numerical columns:\", num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method 1: Standard Scaler\n",
    "for col in num_features:\n",
    "    sc = preprocessing.StandardScaler()\n",
    "    sc.fit(pd.concat([train[[col]], test[[col]]]))\n",
    "    train[col] = sc.transform(train[[col]])\n",
    "    test[col] = sc.transform(test[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# study the skewness in the numerical features\n",
    "skewed_feats = pd.concat([train[num_features], test[num_features]]).apply(lambda x: skew(x.dropna()))\n",
    "print(\"Skew in numeric features:\", skewed_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method 2: Box-Cox transformation when numerical feature skewness > .25\n",
    "for feat in skewed_feats[skewed_feats > 0.25].index:\n",
    "    train[feat], lam = boxcox(train[feat] + 1.)\n",
    "    test[feat], lam = boxcox(test[feat] + 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train.drop(\"loss\", 1), response)\n",
    "dtest = xgb.DMatrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'objective':\"reg:linear\", 'silent': True, 'max_depth': 7, 'min_child_weight': 1,\n",
    "          'colsample_bytree': .7, \"subsample\": 1., 'eta': 0.25, 'eval_metric':'mae',# \"n_estimators\": 20,\n",
    "          \"gamma\": 0.25, \"lambda\": 0.8, \"silent\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvresult = xgb.cv(params, dtrain, nfold=3, num_boost_round=50)\n",
    "print(cvresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvresult[[\"test-mae-mean\", \"train-mae-mean\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvresult[[\"test-mae-mean\", \"train-mae-mean\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.789285\teval-mae:0.791576\n",
      "[1]\ttrain-mae:0.706228\teval-mae:0.708734\n",
      "[2]\ttrain-mae:0.652257\teval-mae:0.655135\n",
      "[3]\ttrain-mae:0.616333\teval-mae:0.619677\n",
      "[4]\ttrain-mae:0.592094\teval-mae:0.595774\n",
      "[5]\ttrain-mae:0.576196\teval-mae:0.580975\n",
      "[6]\ttrain-mae:0.563295\teval-mae:0.568493\n",
      "[7]\ttrain-mae:0.553424\teval-mae:0.559536\n",
      "[8]\ttrain-mae:0.54697\teval-mae:0.55395\n",
      "[9]\ttrain-mae:0.540475\teval-mae:0.5484\n",
      "[10]\ttrain-mae:0.535639\teval-mae:0.544054\n",
      "[11]\ttrain-mae:0.531341\teval-mae:0.540286\n",
      "[12]\ttrain-mae:0.527952\teval-mae:0.537789\n",
      "[13]\ttrain-mae:0.525382\teval-mae:0.535539\n",
      "[14]\ttrain-mae:0.522958\teval-mae:0.533734\n",
      "[15]\ttrain-mae:0.521046\teval-mae:0.532606\n",
      "[16]\ttrain-mae:0.518464\teval-mae:0.530644\n",
      "[17]\ttrain-mae:0.516953\teval-mae:0.529823\n",
      "[18]\ttrain-mae:0.515256\teval-mae:0.528644\n",
      "[19]\ttrain-mae:0.51368\teval-mae:0.527678\n",
      "[20]\ttrain-mae:0.512474\teval-mae:0.527009\n",
      "[21]\ttrain-mae:0.510897\teval-mae:0.526043\n",
      "[22]\ttrain-mae:0.509676\teval-mae:0.5254\n",
      "[23]\ttrain-mae:0.508131\teval-mae:0.524594\n",
      "[24]\ttrain-mae:0.507236\teval-mae:0.524267\n",
      "[25]\ttrain-mae:0.506465\teval-mae:0.52402\n",
      "[26]\ttrain-mae:0.505227\teval-mae:0.523137\n",
      "[27]\ttrain-mae:0.504354\teval-mae:0.522653\n",
      "[28]\ttrain-mae:0.503455\teval-mae:0.522241\n",
      "[29]\ttrain-mae:0.502684\teval-mae:0.522034\n",
      "[30]\ttrain-mae:0.501961\teval-mae:0.521751\n",
      "[31]\ttrain-mae:0.501242\teval-mae:0.521336\n",
      "[32]\ttrain-mae:0.500264\teval-mae:0.520972\n",
      "[33]\ttrain-mae:0.499276\teval-mae:0.5207\n",
      "[34]\ttrain-mae:0.498872\teval-mae:0.520531\n",
      "[35]\ttrain-mae:0.498244\teval-mae:0.520267\n",
      "[36]\ttrain-mae:0.497517\teval-mae:0.519981\n",
      "[37]\ttrain-mae:0.496708\teval-mae:0.519753\n",
      "[38]\ttrain-mae:0.496077\teval-mae:0.519555\n",
      "[39]\ttrain-mae:0.495807\teval-mae:0.519465\n",
      "[40]\ttrain-mae:0.495295\teval-mae:0.519389\n",
      "[41]\ttrain-mae:0.49469\teval-mae:0.519291\n",
      "[42]\ttrain-mae:0.494333\teval-mae:0.519313\n",
      "[43]\ttrain-mae:0.493557\teval-mae:0.519231\n",
      "[44]\ttrain-mae:0.492839\teval-mae:0.518934\n",
      "[45]\ttrain-mae:0.492324\teval-mae:0.518819\n",
      "[46]\ttrain-mae:0.491789\teval-mae:0.518698\n",
      "[47]\ttrain-mae:0.491123\teval-mae:0.518567\n",
      "[48]\ttrain-mae:0.490477\teval-mae:0.518333\n",
      "[49]\ttrain-mae:0.489824\teval-mae:0.518064\n",
      "[50]\ttrain-mae:0.489126\teval-mae:0.517991\n",
      "[51]\ttrain-mae:0.488876\teval-mae:0.517925\n",
      "[52]\ttrain-mae:0.488121\teval-mae:0.517668\n",
      "[53]\ttrain-mae:0.487589\teval-mae:0.517675\n",
      "[54]\ttrain-mae:0.487415\teval-mae:0.517628\n",
      "[55]\ttrain-mae:0.486933\teval-mae:0.517575\n",
      "[56]\ttrain-mae:0.48654\teval-mae:0.517582\n",
      "[57]\ttrain-mae:0.486364\teval-mae:0.517505\n",
      "[58]\ttrain-mae:0.48578\teval-mae:0.517483\n",
      "[59]\ttrain-mae:0.485368\teval-mae:0.517443\n",
      "[60]\ttrain-mae:0.484875\teval-mae:0.517308\n",
      "[61]\ttrain-mae:0.484309\teval-mae:0.517161\n",
      "[62]\ttrain-mae:0.483975\teval-mae:0.517212\n",
      "[63]\ttrain-mae:0.483432\teval-mae:0.517067\n",
      "[64]\ttrain-mae:0.482968\teval-mae:0.51691\n",
      "[65]\ttrain-mae:0.482315\teval-mae:0.516744\n",
      "[66]\ttrain-mae:0.481854\teval-mae:0.516674\n",
      "[67]\ttrain-mae:0.481516\teval-mae:0.51661\n",
      "[68]\ttrain-mae:0.481122\teval-mae:0.516566\n",
      "[69]\ttrain-mae:0.480703\teval-mae:0.516643\n",
      "[70]\ttrain-mae:0.480315\teval-mae:0.516641\n",
      "[71]\ttrain-mae:0.479823\teval-mae:0.516722\n",
      "[72]\ttrain-mae:0.479681\teval-mae:0.516734\n",
      "[73]\ttrain-mae:0.479486\teval-mae:0.516738\n",
      "[74]\ttrain-mae:0.479341\teval-mae:0.516752\n",
      "[75]\ttrain-mae:0.479032\teval-mae:0.51675\n",
      "[76]\ttrain-mae:0.478418\teval-mae:0.516795\n",
      "[77]\ttrain-mae:0.478062\teval-mae:0.516727\n",
      "[78]\ttrain-mae:0.477706\teval-mae:0.516786\n",
      "[79]\ttrain-mae:0.477449\teval-mae:0.516787\n",
      "[80]\ttrain-mae:0.476856\teval-mae:0.516743\n",
      "[81]\ttrain-mae:0.476665\teval-mae:0.51674\n",
      "[82]\ttrain-mae:0.47634\teval-mae:0.516674\n",
      "[83]\ttrain-mae:0.475971\teval-mae:0.516642\n",
      "[84]\ttrain-mae:0.475836\teval-mae:0.516645\n",
      "[85]\ttrain-mae:0.47526\teval-mae:0.516554\n",
      "[86]\ttrain-mae:0.474959\teval-mae:0.516468\n",
      "[87]\ttrain-mae:0.474767\teval-mae:0.516532\n",
      "[88]\ttrain-mae:0.47439\teval-mae:0.516549\n",
      "[89]\ttrain-mae:0.473883\teval-mae:0.51641\n",
      "[90]\ttrain-mae:0.473562\teval-mae:0.516372\n",
      "[91]\ttrain-mae:0.473495\teval-mae:0.516398\n",
      "[92]\ttrain-mae:0.473074\teval-mae:0.516391\n",
      "[93]\ttrain-mae:0.472863\teval-mae:0.516395\n",
      "[94]\ttrain-mae:0.472382\teval-mae:0.516359\n",
      "[95]\ttrain-mae:0.472169\teval-mae:0.516339\n",
      "[96]\ttrain-mae:0.47193\teval-mae:0.516421\n",
      "[97]\ttrain-mae:0.471426\teval-mae:0.516395\n",
      "[98]\ttrain-mae:0.471107\teval-mae:0.516416\n",
      "[99]\ttrain-mae:0.470576\teval-mae:0.516352\n",
      "('best ntree limit', 0, 100)\n",
      "('mae for part train', 0, 1034.6212151560098)\n",
      "('mae for part test', 0, 1160.4503192665534)\n",
      "('mae for all train', 0, 1066.078825270414)\n",
      "[0]\ttrain-mae:0.797679\teval-mae:0.797241\n",
      "[1]\ttrain-mae:0.716859\teval-mae:0.717842\n",
      "[2]\ttrain-mae:0.658275\teval-mae:0.659866\n",
      "[3]\ttrain-mae:0.617202\teval-mae:0.620244\n",
      "[4]\ttrain-mae:0.592536\teval-mae:0.596776\n",
      "[5]\ttrain-mae:0.573517\teval-mae:0.578913\n",
      "[6]\ttrain-mae:0.560597\teval-mae:0.566978\n",
      "[7]\ttrain-mae:0.551252\teval-mae:0.558869\n",
      "[8]\ttrain-mae:0.544367\teval-mae:0.552641\n",
      "[9]\ttrain-mae:0.538649\teval-mae:0.547693\n",
      "[10]\ttrain-mae:0.533651\teval-mae:0.54351\n",
      "[11]\ttrain-mae:0.529807\teval-mae:0.540379\n",
      "[12]\ttrain-mae:0.526284\teval-mae:0.537489\n",
      "[13]\ttrain-mae:0.523504\teval-mae:0.535428\n",
      "[14]\ttrain-mae:0.521225\teval-mae:0.533762\n",
      "[15]\ttrain-mae:0.519342\teval-mae:0.532385\n",
      "[16]\ttrain-mae:0.517723\teval-mae:0.531326\n",
      "[17]\ttrain-mae:0.51546\teval-mae:0.529641\n",
      "[18]\ttrain-mae:0.513904\teval-mae:0.528734\n",
      "[19]\ttrain-mae:0.512703\teval-mae:0.528049\n",
      "[20]\ttrain-mae:0.511445\teval-mae:0.527102\n",
      "[21]\ttrain-mae:0.510315\teval-mae:0.526421\n",
      "[22]\ttrain-mae:0.509153\teval-mae:0.525694\n",
      "[23]\ttrain-mae:0.508257\teval-mae:0.525226\n",
      "[24]\ttrain-mae:0.507147\teval-mae:0.524717\n",
      "[25]\ttrain-mae:0.50623\teval-mae:0.524267\n",
      "[26]\ttrain-mae:0.505185\teval-mae:0.523833\n",
      "[27]\ttrain-mae:0.504523\teval-mae:0.523819\n",
      "[28]\ttrain-mae:0.503818\teval-mae:0.523503\n",
      "[29]\ttrain-mae:0.502899\teval-mae:0.52302\n",
      "[30]\ttrain-mae:0.502171\teval-mae:0.522824\n",
      "[31]\ttrain-mae:0.501484\teval-mae:0.522401\n",
      "[32]\ttrain-mae:0.500667\teval-mae:0.522208\n",
      "[33]\ttrain-mae:0.499983\teval-mae:0.521857\n",
      "[34]\ttrain-mae:0.499328\teval-mae:0.52178\n",
      "[35]\ttrain-mae:0.498437\teval-mae:0.521483\n",
      "[36]\ttrain-mae:0.497646\teval-mae:0.52122\n",
      "[37]\ttrain-mae:0.496846\teval-mae:0.521068\n",
      "[38]\ttrain-mae:0.496359\teval-mae:0.520909\n",
      "[39]\ttrain-mae:0.49596\teval-mae:0.520895\n",
      "[40]\ttrain-mae:0.49555\teval-mae:0.520867\n",
      "[41]\ttrain-mae:0.495048\teval-mae:0.520746\n",
      "[42]\ttrain-mae:0.494411\teval-mae:0.52063\n",
      "[43]\ttrain-mae:0.494019\teval-mae:0.520439\n",
      "[44]\ttrain-mae:0.49329\teval-mae:0.520287\n",
      "[45]\ttrain-mae:0.492381\teval-mae:0.519884\n",
      "[46]\ttrain-mae:0.491933\teval-mae:0.519835\n",
      "[47]\ttrain-mae:0.491487\teval-mae:0.519679\n",
      "[48]\ttrain-mae:0.491261\teval-mae:0.519667\n",
      "[49]\ttrain-mae:0.490628\teval-mae:0.519666\n",
      "[50]\ttrain-mae:0.490125\teval-mae:0.519609\n",
      "[51]\ttrain-mae:0.48979\teval-mae:0.519543\n",
      "[52]\ttrain-mae:0.489371\teval-mae:0.51952\n",
      "[53]\ttrain-mae:0.488547\teval-mae:0.519212\n",
      "[54]\ttrain-mae:0.488125\teval-mae:0.519196\n",
      "[55]\ttrain-mae:0.48767\teval-mae:0.519158\n",
      "[56]\ttrain-mae:0.487254\teval-mae:0.51927\n",
      "[57]\ttrain-mae:0.486891\teval-mae:0.519233\n",
      "[58]\ttrain-mae:0.486138\teval-mae:0.519089\n",
      "[59]\ttrain-mae:0.485904\teval-mae:0.519089\n",
      "[60]\ttrain-mae:0.485313\teval-mae:0.519083\n",
      "[61]\ttrain-mae:0.484727\teval-mae:0.519032\n",
      "[62]\ttrain-mae:0.484145\teval-mae:0.519002\n",
      "[63]\ttrain-mae:0.48403\teval-mae:0.519003\n",
      "[64]\ttrain-mae:0.48374\teval-mae:0.518929\n",
      "[65]\ttrain-mae:0.483297\teval-mae:0.518951\n",
      "[66]\ttrain-mae:0.483116\teval-mae:0.518928\n",
      "[67]\ttrain-mae:0.482919\teval-mae:0.518828\n",
      "[68]\ttrain-mae:0.482728\teval-mae:0.518846\n",
      "[69]\ttrain-mae:0.482499\teval-mae:0.518837\n",
      "[70]\ttrain-mae:0.481967\teval-mae:0.518796\n",
      "[71]\ttrain-mae:0.481639\teval-mae:0.518831\n",
      "[72]\ttrain-mae:0.481254\teval-mae:0.518845\n",
      "[73]\ttrain-mae:0.480925\teval-mae:0.518841\n",
      "[74]\ttrain-mae:0.480504\teval-mae:0.518854\n",
      "[75]\ttrain-mae:0.480259\teval-mae:0.518838\n",
      "[76]\ttrain-mae:0.479535\teval-mae:0.518687\n",
      "[77]\ttrain-mae:0.479143\teval-mae:0.518624\n",
      "[78]\ttrain-mae:0.478487\teval-mae:0.5185\n",
      "[79]\ttrain-mae:0.478045\teval-mae:0.518491\n",
      "[80]\ttrain-mae:0.477647\teval-mae:0.518538\n",
      "[81]\ttrain-mae:0.477008\teval-mae:0.518432\n",
      "[82]\ttrain-mae:0.47667\teval-mae:0.518542\n",
      "[83]\ttrain-mae:0.476265\teval-mae:0.518559\n",
      "[84]\ttrain-mae:0.476196\teval-mae:0.51856\n",
      "[85]\ttrain-mae:0.475897\teval-mae:0.518502\n",
      "[86]\ttrain-mae:0.475699\teval-mae:0.518487\n",
      "[87]\ttrain-mae:0.475196\teval-mae:0.518515\n",
      "[88]\ttrain-mae:0.475043\teval-mae:0.518535\n",
      "[89]\ttrain-mae:0.474698\teval-mae:0.518558\n",
      "[90]\ttrain-mae:0.474486\teval-mae:0.518632\n",
      "[91]\ttrain-mae:0.474215\teval-mae:0.518657\n",
      "[92]\ttrain-mae:0.473848\teval-mae:0.51872\n",
      "[93]\ttrain-mae:0.473189\teval-mae:0.518654\n",
      "[94]\ttrain-mae:0.472777\teval-mae:0.518615\n",
      "[95]\ttrain-mae:0.472332\teval-mae:0.518621\n",
      "[96]\ttrain-mae:0.472021\teval-mae:0.518525\n",
      "[97]\ttrain-mae:0.471705\teval-mae:0.518468\n",
      "[98]\ttrain-mae:0.471619\teval-mae:0.518448\n",
      "[99]\ttrain-mae:0.471393\teval-mae:0.51852\n",
      "('best ntree limit', 1, 100)\n",
      "('mae for part train', 1, 1036.3294161492606)\n",
      "('mae for part test', 1, 1169.9243733721194)\n",
      "('mae for all train', 1, 1069.728510160732)\n",
      "[0]\ttrain-mae:0.790749\teval-mae:0.794592\n",
      "[1]\ttrain-mae:0.706171\teval-mae:0.710394\n",
      "[2]\ttrain-mae:0.651783\teval-mae:0.656911\n",
      "[3]\ttrain-mae:0.614733\teval-mae:0.620732\n",
      "[4]\ttrain-mae:0.590671\teval-mae:0.5971\n",
      "[5]\ttrain-mae:0.573483\teval-mae:0.580443\n",
      "[6]\ttrain-mae:0.561518\teval-mae:0.569162\n",
      "[7]\ttrain-mae:0.55176\teval-mae:0.559879\n",
      "[8]\ttrain-mae:0.544786\teval-mae:0.553592\n",
      "[9]\ttrain-mae:0.538825\teval-mae:0.548571\n",
      "[10]\ttrain-mae:0.534112\teval-mae:0.544655\n",
      "[11]\ttrain-mae:0.530643\teval-mae:0.541943\n",
      "[12]\ttrain-mae:0.526606\teval-mae:0.538377\n",
      "[13]\ttrain-mae:0.524056\teval-mae:0.53652\n",
      "[14]\ttrain-mae:0.521703\teval-mae:0.534936\n",
      "[15]\ttrain-mae:0.5196\teval-mae:0.533341\n",
      "[16]\ttrain-mae:0.517714\teval-mae:0.532021\n",
      "[17]\ttrain-mae:0.516254\teval-mae:0.531051\n",
      "[18]\ttrain-mae:0.514298\teval-mae:0.529741\n",
      "[19]\ttrain-mae:0.512933\teval-mae:0.52902\n",
      "[20]\ttrain-mae:0.511551\teval-mae:0.528087\n",
      "[21]\ttrain-mae:0.51025\teval-mae:0.527411\n",
      "[22]\ttrain-mae:0.509082\teval-mae:0.526694\n",
      "[23]\ttrain-mae:0.507605\teval-mae:0.525877\n",
      "[24]\ttrain-mae:0.506851\teval-mae:0.525526\n",
      "[25]\ttrain-mae:0.505844\teval-mae:0.525042\n",
      "[26]\ttrain-mae:0.504984\teval-mae:0.524732\n",
      "[27]\ttrain-mae:0.504168\teval-mae:0.524433\n",
      "[28]\ttrain-mae:0.503128\teval-mae:0.52386\n",
      "[29]\ttrain-mae:0.502308\teval-mae:0.523535\n",
      "[30]\ttrain-mae:0.501252\teval-mae:0.523037\n",
      "[31]\ttrain-mae:0.500433\teval-mae:0.522612\n",
      "[32]\ttrain-mae:0.499375\teval-mae:0.522076\n",
      "[33]\ttrain-mae:0.498536\teval-mae:0.521776\n",
      "[34]\ttrain-mae:0.497838\teval-mae:0.521612\n",
      "[35]\ttrain-mae:0.497309\teval-mae:0.521496\n",
      "[36]\ttrain-mae:0.496758\teval-mae:0.521259\n",
      "[37]\ttrain-mae:0.496225\teval-mae:0.520999\n",
      "[38]\ttrain-mae:0.495746\teval-mae:0.52087\n",
      "[39]\ttrain-mae:0.494954\teval-mae:0.520666\n",
      "[40]\ttrain-mae:0.494443\teval-mae:0.520483\n",
      "[41]\ttrain-mae:0.494095\teval-mae:0.520297\n",
      "[42]\ttrain-mae:0.493729\teval-mae:0.520316\n",
      "[43]\ttrain-mae:0.493084\teval-mae:0.520237\n",
      "[44]\ttrain-mae:0.492659\teval-mae:0.520208\n",
      "[45]\ttrain-mae:0.491998\teval-mae:0.520029\n",
      "[46]\ttrain-mae:0.491502\teval-mae:0.519936\n",
      "[47]\ttrain-mae:0.490815\teval-mae:0.519901\n",
      "[48]\ttrain-mae:0.490472\teval-mae:0.519869\n",
      "[49]\ttrain-mae:0.490013\teval-mae:0.519781\n",
      "[50]\ttrain-mae:0.489254\teval-mae:0.519485\n",
      "[51]\ttrain-mae:0.488655\teval-mae:0.519344\n",
      "[52]\ttrain-mae:0.488416\teval-mae:0.519324\n",
      "[53]\ttrain-mae:0.487821\teval-mae:0.519216\n",
      "[54]\ttrain-mae:0.487318\teval-mae:0.519021\n",
      "[55]\ttrain-mae:0.486652\teval-mae:0.518994\n",
      "[56]\ttrain-mae:0.485984\teval-mae:0.518969\n",
      "[57]\ttrain-mae:0.485649\teval-mae:0.51897\n",
      "[58]\ttrain-mae:0.485433\teval-mae:0.519012\n",
      "[59]\ttrain-mae:0.484824\teval-mae:0.518916\n",
      "[60]\ttrain-mae:0.484312\teval-mae:0.518766\n",
      "[61]\ttrain-mae:0.483909\teval-mae:0.518844\n",
      "[62]\ttrain-mae:0.483524\teval-mae:0.518721\n",
      "[63]\ttrain-mae:0.483342\teval-mae:0.518649\n",
      "[64]\ttrain-mae:0.483172\teval-mae:0.518642\n",
      "[65]\ttrain-mae:0.482904\teval-mae:0.518622\n",
      "[66]\ttrain-mae:0.482742\teval-mae:0.518599\n",
      "[67]\ttrain-mae:0.482359\teval-mae:0.518644\n",
      "[68]\ttrain-mae:0.482105\teval-mae:0.518709\n",
      "[69]\ttrain-mae:0.481731\teval-mae:0.518684\n",
      "[70]\ttrain-mae:0.48118\teval-mae:0.518655\n",
      "[71]\ttrain-mae:0.481039\teval-mae:0.518672\n",
      "[72]\ttrain-mae:0.480616\teval-mae:0.518547\n",
      "[73]\ttrain-mae:0.479892\teval-mae:0.518503\n",
      "[74]\ttrain-mae:0.479215\teval-mae:0.51827\n",
      "[75]\ttrain-mae:0.478784\teval-mae:0.518331\n",
      "[76]\ttrain-mae:0.478634\teval-mae:0.518316\n",
      "[77]\ttrain-mae:0.478318\teval-mae:0.518222\n",
      "[78]\ttrain-mae:0.477851\teval-mae:0.518148\n",
      "[79]\ttrain-mae:0.47765\teval-mae:0.518135\n",
      "[80]\ttrain-mae:0.47741\teval-mae:0.5181\n",
      "[81]\ttrain-mae:0.476962\teval-mae:0.518158\n",
      "[82]\ttrain-mae:0.47673\teval-mae:0.518106\n",
      "[83]\ttrain-mae:0.476217\teval-mae:0.517992\n",
      "[84]\ttrain-mae:0.475881\teval-mae:0.51807\n",
      "[85]\ttrain-mae:0.475575\teval-mae:0.518064\n",
      "[86]\ttrain-mae:0.47522\teval-mae:0.518062\n",
      "[87]\ttrain-mae:0.474667\teval-mae:0.518031\n",
      "[88]\ttrain-mae:0.474229\teval-mae:0.51804\n",
      "[89]\ttrain-mae:0.473867\teval-mae:0.518062\n",
      "[90]\ttrain-mae:0.473637\teval-mae:0.518057\n",
      "[91]\ttrain-mae:0.473416\teval-mae:0.51813\n",
      "[92]\ttrain-mae:0.47294\teval-mae:0.518091\n",
      "[93]\ttrain-mae:0.4727\teval-mae:0.518034\n",
      "[94]\ttrain-mae:0.472337\teval-mae:0.517994\n",
      "[95]\ttrain-mae:0.471964\teval-mae:0.518\n",
      "[96]\ttrain-mae:0.47162\teval-mae:0.517963\n",
      "[97]\ttrain-mae:0.47133\teval-mae:0.517944\n",
      "[98]\ttrain-mae:0.470957\teval-mae:0.518041\n",
      "[99]\ttrain-mae:0.470674\teval-mae:0.518033\n",
      "('best ntree limit', 2, 100)\n",
      "('mae for part train', 2, 1035.0538540221594)\n",
      "('mae for part test', 2, 1158.428267833752)\n",
      "('mae for all train', 2, 1065.8971299056966)\n",
      "[0]\ttrain-mae:0.790447\teval-mae:0.787873\n",
      "[1]\ttrain-mae:0.707175\teval-mae:0.706149\n",
      "[2]\ttrain-mae:0.653975\teval-mae:0.653875\n",
      "[3]\ttrain-mae:0.615935\teval-mae:0.617033\n",
      "[4]\ttrain-mae:0.59112\teval-mae:0.593374\n",
      "[5]\ttrain-mae:0.574838\teval-mae:0.578105\n",
      "[6]\ttrain-mae:0.563157\teval-mae:0.567518\n",
      "[7]\ttrain-mae:0.554003\teval-mae:0.55902\n",
      "[8]\ttrain-mae:0.54619\teval-mae:0.552145\n",
      "[9]\ttrain-mae:0.541032\teval-mae:0.547692\n",
      "[10]\ttrain-mae:0.535682\teval-mae:0.54307\n",
      "[11]\ttrain-mae:0.531596\teval-mae:0.539677\n",
      "[12]\ttrain-mae:0.527962\teval-mae:0.536545\n",
      "[13]\ttrain-mae:0.525424\teval-mae:0.53457\n",
      "[14]\ttrain-mae:0.522727\teval-mae:0.532661\n",
      "[15]\ttrain-mae:0.519925\teval-mae:0.530352\n",
      "[16]\ttrain-mae:0.518009\teval-mae:0.529042\n",
      "[17]\ttrain-mae:0.516272\teval-mae:0.527877\n",
      "[18]\ttrain-mae:0.514531\teval-mae:0.526775\n",
      "[19]\ttrain-mae:0.513051\teval-mae:0.526049\n",
      "[20]\ttrain-mae:0.511672\teval-mae:0.525297\n",
      "[21]\ttrain-mae:0.510493\teval-mae:0.524807\n",
      "[22]\ttrain-mae:0.509613\teval-mae:0.524213\n",
      "[23]\ttrain-mae:0.508633\teval-mae:0.523604\n",
      "[24]\ttrain-mae:0.50763\teval-mae:0.523058\n",
      "[25]\ttrain-mae:0.506297\teval-mae:0.522241\n",
      "[26]\ttrain-mae:0.505643\teval-mae:0.522021\n",
      "[27]\ttrain-mae:0.504747\teval-mae:0.521491\n",
      "[28]\ttrain-mae:0.503895\teval-mae:0.52109\n",
      "[29]\ttrain-mae:0.502991\teval-mae:0.520872\n",
      "[30]\ttrain-mae:0.502192\teval-mae:0.520565\n",
      "[31]\ttrain-mae:0.501454\teval-mae:0.520184\n",
      "[32]\ttrain-mae:0.500786\teval-mae:0.519881\n",
      "[33]\ttrain-mae:0.500047\teval-mae:0.519693\n",
      "[34]\ttrain-mae:0.499559\teval-mae:0.519522\n",
      "[35]\ttrain-mae:0.498632\teval-mae:0.519088\n",
      "[36]\ttrain-mae:0.498135\teval-mae:0.518908\n",
      "[37]\ttrain-mae:0.49763\teval-mae:0.518809\n",
      "[38]\ttrain-mae:0.496578\teval-mae:0.518414\n",
      "[39]\ttrain-mae:0.49583\teval-mae:0.518194\n",
      "[40]\ttrain-mae:0.495329\teval-mae:0.518041\n",
      "[41]\ttrain-mae:0.494594\teval-mae:0.518003\n",
      "[42]\ttrain-mae:0.493916\teval-mae:0.517813\n",
      "[43]\ttrain-mae:0.493432\teval-mae:0.517792\n",
      "[44]\ttrain-mae:0.492677\teval-mae:0.517657\n",
      "[45]\ttrain-mae:0.492313\teval-mae:0.517574\n",
      "[46]\ttrain-mae:0.49186\teval-mae:0.517333\n",
      "[47]\ttrain-mae:0.49162\teval-mae:0.517392\n",
      "[48]\ttrain-mae:0.491102\teval-mae:0.51732\n",
      "[49]\ttrain-mae:0.490746\teval-mae:0.517279\n",
      "[50]\ttrain-mae:0.490361\teval-mae:0.517247\n",
      "[51]\ttrain-mae:0.489579\teval-mae:0.51704\n",
      "[52]\ttrain-mae:0.488674\teval-mae:0.516662\n",
      "[53]\ttrain-mae:0.487838\teval-mae:0.516525\n",
      "[54]\ttrain-mae:0.487334\teval-mae:0.516442\n",
      "[55]\ttrain-mae:0.486867\teval-mae:0.516446\n",
      "[56]\ttrain-mae:0.486406\teval-mae:0.516387\n",
      "[57]\ttrain-mae:0.486003\teval-mae:0.516311\n",
      "[58]\ttrain-mae:0.485673\teval-mae:0.51624\n",
      "[59]\ttrain-mae:0.485176\teval-mae:0.516195\n",
      "[60]\ttrain-mae:0.484617\teval-mae:0.516086\n",
      "[61]\ttrain-mae:0.484302\teval-mae:0.516012\n",
      "[62]\ttrain-mae:0.483841\teval-mae:0.515999\n",
      "[63]\ttrain-mae:0.483454\teval-mae:0.516056\n",
      "[64]\ttrain-mae:0.483065\teval-mae:0.516006\n",
      "[65]\ttrain-mae:0.482887\teval-mae:0.515887\n",
      "[66]\ttrain-mae:0.482583\teval-mae:0.515825\n",
      "[67]\ttrain-mae:0.482122\teval-mae:0.515855\n",
      "[68]\ttrain-mae:0.48186\teval-mae:0.51581\n",
      "[69]\ttrain-mae:0.481391\teval-mae:0.51581\n",
      "[70]\ttrain-mae:0.481156\teval-mae:0.51587\n",
      "[71]\ttrain-mae:0.480866\teval-mae:0.515883\n",
      "[72]\ttrain-mae:0.480541\teval-mae:0.515902\n",
      "[73]\ttrain-mae:0.480413\teval-mae:0.515925\n",
      "[74]\ttrain-mae:0.479961\teval-mae:0.515842\n",
      "[75]\ttrain-mae:0.479314\teval-mae:0.515613\n",
      "[76]\ttrain-mae:0.478891\teval-mae:0.515655\n",
      "[77]\ttrain-mae:0.478463\teval-mae:0.515613\n",
      "[78]\ttrain-mae:0.478342\teval-mae:0.515642\n",
      "[79]\ttrain-mae:0.477969\teval-mae:0.515566\n",
      "[80]\ttrain-mae:0.477647\teval-mae:0.515531\n",
      "[81]\ttrain-mae:0.477184\teval-mae:0.515539\n",
      "[82]\ttrain-mae:0.476751\teval-mae:0.515582\n",
      "[83]\ttrain-mae:0.476576\teval-mae:0.515557\n",
      "[84]\ttrain-mae:0.47597\teval-mae:0.515422\n",
      "[85]\ttrain-mae:0.47576\teval-mae:0.515471\n",
      "[86]\ttrain-mae:0.475421\teval-mae:0.5155\n",
      "[87]\ttrain-mae:0.475118\teval-mae:0.515491\n",
      "[88]\ttrain-mae:0.474564\teval-mae:0.515484\n",
      "[89]\ttrain-mae:0.474406\teval-mae:0.515484\n",
      "[90]\ttrain-mae:0.474003\teval-mae:0.515427\n",
      "[91]\ttrain-mae:0.473577\teval-mae:0.515351\n",
      "[92]\ttrain-mae:0.473436\teval-mae:0.51535\n",
      "[93]\ttrain-mae:0.473131\teval-mae:0.51539\n",
      "[94]\ttrain-mae:0.472919\teval-mae:0.515371\n",
      "[95]\ttrain-mae:0.472884\teval-mae:0.515358\n",
      "[96]\ttrain-mae:0.472623\teval-mae:0.515344\n",
      "[97]\ttrain-mae:0.472333\teval-mae:0.515317\n",
      "[98]\ttrain-mae:0.472094\teval-mae:0.515328\n",
      "[99]\ttrain-mae:0.471812\teval-mae:0.515356\n",
      "('best ntree limit', 3, 100)\n",
      "('mae for part train', 3, 1037.8312032441142)\n",
      "('mae for part test', 3, 1155.0088520356026)\n",
      "('mae for all train', 3, 1067.1253043255533)\n"
     ]
    }
   ],
   "source": [
    "folds = 4\n",
    "\n",
    "pred_test = 0.\n",
    "pred_train = 0.\n",
    "restored_pred_train = 0.\n",
    "restored_pred_test = 0.\n",
    "\n",
    "kf = KFold(n_splits=folds)\n",
    "kf.split(train)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "    train_pd_ind = train.index[train_index]\n",
    "    test_pd_ind = train.index[test_index]\n",
    "    train_part, test_part = train.ix[train_pd_ind], train.ix[test_pd_ind]\n",
    "    \n",
    "    dtrain_part = xgb.DMatrix(train_part.drop(\"loss\", 1), response[train_pd_ind])\n",
    "    dtest_part = xgb.DMatrix(test_part.drop(\"loss\", 1), response[test_pd_ind])\n",
    "    params['seed'] = i * 5 + 100\n",
    "    clf = xgb.train(params, dtrain_part, num_boost_round=100,\n",
    "                    evals=[(dtrain_part, \"train\"), (dtest_part, \"eval\")])\n",
    "    \n",
    "    print(\"best ntree limit\", i, clf.best_ntree_limit)\n",
    "    this_pred_train = clf.predict(dtrain, ntree_limit=clf.best_ntree_limit)\n",
    "    print(\"mae for part train\",i, mean_absolute_error(\n",
    "            train_part.loss, restore_pred(clf.predict(dtrain_part, ntree_limit=clf.best_ntree_limit))))\n",
    "    print(\"mae for part test\",i, mean_absolute_error(\n",
    "            test_part.loss, restore_pred(clf.predict(dtest_part, ntree_limit=clf.best_ntree_limit))))\n",
    "    print(\"mae for all train\",i, mean_absolute_error(train.loss, restore_pred(this_pred_train)))\n",
    "    \n",
    "    pred_train += this_pred_train\n",
    "    restored_pred_train += restore_pred(this_pred_train)\n",
    "    \n",
    "    this_pred_test = clf.predict(dtest, ntree_limit=clf.best_ntree_limit)\n",
    "    pred_test += this_pred_test\n",
    "    restored_pred_test += restore_pred(this_pred_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mae final restore after', 1050.1802405758608)\n",
      "('mae final restore before', 1048.9870091640296)\n"
     ]
    }
   ],
   "source": [
    "print(\"mae final restore after\", mean_absolute_error(train.loss, restore_pred(pred_train / folds)))\n",
    "print(\"mae final restore before\", mean_absolute_error(train.loss, restored_pred_train / folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "result = pd.DataFrame({\"id\": test.index, \"loss\": restored_pred_test / folds})\n",
    "result.to_csv(\"result_restored_before{:%Y%m%d%H%-M}.csv\".format(datetime.datetime.now()), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using XGBRegressor and important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params_reg = dict(params)\n",
    "params_reg.pop(\"eta\")\n",
    "params_reg.pop('eval_metric')\n",
    "params_reg.pop('lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = XGBRegressor(**params_reg)\n",
    "reg.fit(train.drop(\"loss\", 1), train.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_predprob = reg.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg_booster = reg.booster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figsize(18, 5)\n",
    "feat_imp = pd.Series(reg.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "important_features = list(feat_imp[feat_imp > 4].index)\n",
    "print(\"important features:\", important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain_imp = xgb.DMatrix(train[important_features], train.loss)\n",
    "cvresult = xgb.cv(params, dtrain_imp, nfold=4, num_boost_round=50)\n",
    "print(cvresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params2 = {'base_score': 0.1, 'colsample_bytree': 0.9,\n",
    " 'eta': 0.3,\n",
    " 'eval_metric': 'mae',\n",
    " 'max_depth': 7,\n",
    " 'min_child_weight': 3,\n",
    " 'n_estimators': 10,\n",
    " 'objective': 'reg:linear',\n",
    " 'seed': 1,\n",
    " 'silent': True}\n",
    "regb = xgb.train(params2, dtrain_imp, num_boost_round=50, evals=[(dtrain_imp, \"train\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator=reg, \n",
    " param_grid = param_test1, scoring='neg_mean_squared_error',n_jobs=4, iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsearch1.fit(train.drop(\"loss\", 1), train.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
